#!/usr/bin/env python3
# https://ai.google.dev/
# pip install Proxy-List-Scrapper
# pip install langcodes[data]


import concurrent.futures
import base64
import random
import threading
import time
import requests
import traceback

import langcodes
from sqlitedict import SqliteDict

import cfg
import my_dic
import my_google
import my_log
import my_proxy


STOP_DAEMON = False


# –∫–∞–∂–¥—ã–π —é–∑–µ—Ä –¥–∞–µ—Ç —Å–≤–æ–∏ –∫–ª—é—á–∏ –∏ –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å–æ –≤—Å–µ–º–∏
# –∫–∞–∂–¥—ã–π –∫–ª—é—á –¥–∞–µ—Ç –≤—Å–µ–≥–æ 50 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –¥–µ–Ω—å —Ç–∞–∫ —á—Ç–æ —á–µ–º –±–æ–ª—å—à–µ —Ç–µ–º –ª—É—á—à–µ
# –¥—Ä—É–≥–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è - 32–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º–∏–Ω—É—Ç—É, 2 –∑–∞–ø—Ä–æ—Å–∞ –≤ –º–∏–Ω—É—Ç—É
# {full_chat_id as str: list of keys as list of str}
# {'[2654672534] [0]': ['key1','key2','key3'], ...}
USER_KEYS = SqliteDict('db/gemini_user_keys.db', autocommit=True)
# list of all users keys
ALL_KEYS = []
USER_KEYS_LOCK = threading.Lock()


# –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ gemini
TIMEOUT = 120


# –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —á–∞—Ç–æ–≤ —á—Ç–æ –±—ã –Ω–µ –∏—Å–ø–æ—Ä—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é 
# {id:lock}
LOCKS = {}

# memory save lock
SAVE_LOCK = threading.Lock()

# –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã –±–æ–ª—å—à–µ —á–µ–º, —ç—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç–∞, –≤ —ç—Ç–æ–º –º–æ–¥—É–ª–µ –æ–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
MAX_REQUEST = 25000

# –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏ (32–∫ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Google?)
# MAX_CHAT_SIZE = 25000
MAX_CHAT_SIZE = 31000
# —Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–º–Ω–∏—Ç—å, –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ (–î–æ–ª–∂–Ω–æ –±—ã—Ç—å >2 –∏ –∫—Ä–∞—Ç–Ω–æ 2)
# 20 - –∑–Ω–∞—á–∏—Ç –ø–æ–º–Ω–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤
MAX_CHAT_LINES = 20
if hasattr(cfg, 'GEMINI_MAX_CHAT_LINES'):
    MAX_CHAT_LINES = cfg.GEMINI_MAX_CHAT_LINES

# –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å 2 –∑–∞–ø—Ä–æ—Å–∞ –ø–æ 15000 –≤ —Å—É–º–º–µ –ø–æ–ª—É—á–∏—Ç—Å—è –∑–∞–ø—Ä–æ—Å —Ä–∞–∑–º–µ—Ä–æ–º 30000
# –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Å—É–º–º–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤
MAX_SUM_REQUEST = 150000
# MAX_SUM_REQUEST = 31000

# —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–∏–∞–ª–æ–≥–æ–≤ {id:list(mem)}
CHATS = SqliteDict('db/gemini_dialogs.db', autocommit=True)

# magic string
CANDIDATES = '78fgh892890df@d7gkln2937DHf98723Dgh'


##################################################################################
# If no proxies are specified in the config, then we first try to work directly
# and if that doesn't work, we start looking for free proxies using
# a constantly running daemon
PROXY_POOL = my_dic.PersistentList('db/gemini_proxy_pool_v2.pkl')
PROXY_POLL_SPEED = SqliteDict('db/gemini_proxy_pool_speed_v2.pkl')
# PROXY_POOL_REMOVED = my_dic.PersistentList('db/gemini_proxy_pool_removed_v2.pkl')
PROXY_POOL_REMOVED = [] # –Ω–µ –Ω–∞–¥–æ –Ω–∞–≤–µ—Ä–Ω–æ–µ –ø–æ–º–Ω–∏—Ç—å –≤—Å–µ–≥–¥–∞ –≤—Å–µ —É–¥–∞–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–∫—Å–∏

# –∏—Å–∫–∞—Ç—å –∏ –¥–æ–±–∞–≤–ª—è—Ç—å –ø—Ä–æ–∫—Å–∏ –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥–µ—Ç—Å—è —Ö–æ—Ç—è –±—ã 10 –ø—Ä–æ–∫—Å–µ–π
MAX_PROXY_POOL = 10
# –Ω–∞—á–∏–Ω–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–æ—Å—å –≤—Å–µ–≥–æ 5 –ø—Ä–æ–∫—Å–µ–π
MAX_PROXY_POOL_LOW_MARGIN = 5

SAVE_LOCK = threading.Lock()
POOL_MAX_WORKERS = 50
##################################################################################


def img2txt(data_: bytes, prompt: str = "–ß—Ç–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ, –ø–æ–¥—Ä–æ–±–Ω–æ?") -> str:
    """
    Generates a textual description of an image based on its contents.

    Args:
        data_: The image data as bytes.
        prompt: The prompt to provide for generating the description. Defaults to "–ß—Ç–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ, –ø–æ–¥—Ä–æ–±–Ω–æ?".

    Returns:
        A textual description of the image.

    Raises:
        None.
    """
    global PROXY_POOL
    try:
        img_data = base64.b64encode(data_).decode("utf-8")
        data = {
            "contents": [
                {
                "parts": [
                    {"text": prompt},
                    {
                    "inline_data": {
                        "mime_type": "image/jpeg",
                        "data": img_data
                    }
                    }
                ]
                }
            ]
            }

        result = ''
        keys = cfg.gemini_keys[:]  + ALL_KEYS
        random.shuffle(keys)

        proxies = PROXY_POOL[:]
        random.shuffle(proxies)

        for api_key in keys:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro-vision:generateContent?key={api_key}"

            if proxies:
                sort_proxies_by_speed(proxies)
                for proxy in proxies:
                    start_time = time.time()
                    session = requests.Session()
                    session.proxies = {"http": proxy, "https": proxy}
                    try:
                        response = session.post(url, json=data, timeout=TIMEOUT).json()
                        try:
                            result = response['candidates'][0]['content']['parts'][0]['text']
                            if result == '' or result:
                                return result.strip()
                        except Exception as error_ca:
                            if 'candidates' in str(error_ca) or 'content' in str(error_ca):
                                my_log.log2(f'my_gemini:img2txt:{error_ca}')
                                return ''
                        if result:
                            end_time = time.time()
                            total_time = end_time - start_time
                            if total_time > 45:
                                remove_proxy(proxy)
                            break
                        if result == '':
                            break
                    except (requests.exceptions.ProxyError, requests.exceptions.ConnectionError) as error:
                        remove_proxy(proxy)
                        continue
            else:
                try:
                    response = requests.post(url, json=data, timeout=TIMEOUT).json()
                    try:
                        result = response['candidates'][0]['content']['parts'][0]['text']
                        if result == '' or result:
                            return result.strip()
                    except Exception as error_ca:
                        if 'candidates' in str(error_ca) or 'content' in str(error_ca):
                            my_log.log2(f'my_gemini:img2txt:{error_ca}')
                            return ''
                except Exception as error:
                    if 'content' in str(error):
                        return ''
                    my_log.log2(f'img2txt:{error}')
        return result.strip()
    except Exception as unknown_error:
        if 'content' not in str(error):
            my_log.log2(f'my_gemini:img2txt:{unknown_error}')
        return ''


def update_mem(query: str, resp: str, mem):
    """
    Update the memory with the given query and response.

    Parameters:
        query (str): The input query.
        resp (str): The response to the query.
        mem: The memory object to update, if str than mem is a chat_id

    Returns:
        list: The updated memory object.
    """
    global CHATS
    chat_id = ''
    if isinstance(mem, str): # if mem - chat_id
        chat_id = mem
        if mem not in CHATS:
            CHATS[mem] = []
        mem = CHATS[mem]

    mem.append({"role": "user", "parts": [{"text": query}]})
    mem.append({"role": "model", "parts": [{"text": resp}]})
    size = 0
    for x in mem:
        text = x['parts'][0]['text']
        size += len(text)
    while size > MAX_CHAT_SIZE:
        mem = mem[2:]
        size = 0
        for x in mem:
            text = x['parts'][0]['text']
            size += len(text)
    mem = mem[-MAX_CHAT_LINES:]
    if chat_id:
        CHATS[chat_id] = mem
    return mem


def undo(chat_id: str):
    """
    Undo the last two lines of chat history for a given chat ID.

    Args:
        chat_id (str): The ID of the chat.

    Raises:
        Exception: If there is an error while undoing the chat history.

    Returns:
        None
    """
    try:
        global LOCKS, CHATS

        if chat_id in LOCKS:
            lock = LOCKS[chat_id]
        else:
            lock = threading.Lock()
            LOCKS[chat_id] = lock
        with lock:
            if chat_id in CHATS:
                mem = CHATS[chat_id]
                # remove 2 last lines from mem
                mem = mem[:-2]
                CHATS[chat_id] = mem
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_gemini(f'Failed to undo chat {chat_id}: {error}\n\n{error_traceback}')


def remove_key(key: str):
    """
    Removes a given key from the ALL_KEYS list and from the USER_KEYS dictionary.
    
    Args:
        key (str): The key to be removed.
        
    Returns:
        None
    """
    try:
        if key in ALL_KEYS:
            del ALL_KEYS[ALL_KEYS.index(key)]
        with USER_KEYS_LOCK:
            # remove key from USER_KEYS
            for user in USER_KEYS:
                if key in USER_KEYS[user]:
                    USER_KEYS[user] = [x for x in USER_KEYS[user] if x != key]
                    my_log.log_gemini(f'Invalid key {key} removed from user {user}')
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_gemini(f'Failed to remove key {key}: {error}\n\n{error_traceback}')


def ai(q: str, mem = [],
       temperature: float = 0.1,
       proxy_str: str = '',
       model: str = '',
       key__: str = None,
       tokens_limit: int = 8000) -> str:
    """
    Generates a response to a given question using the Generative AI model.
    
    Args:
        q (str): The question to be answered.
        mem (list, optional): The memory to be used for generating the response. Defaults to [].
        temperature (float, optional): The temperature parameter for the model. Defaults to 0.1.
        proxy_str (str, optional): The proxy to be used for the request. Defaults to ''.
        model (str, optional): The model to be used for generating the response. Defaults to ''.
        key__ (str, optional): The API key to be used for the request. Defaults to None.
        
    Returns:
        str: The generated response to the question.
        
    Raises:
        Exception: If an error occurs during the request or response handling.
    """
    if model == '':
        model = 'gemini-1.5-flash-latest'
        # models/gemini-1.0-pro
        # models/gemini-1.0-pro-001
        # models/gemini-1.0-pro-latest
        # models/gemini-1.0-pro-vision-latest
        # models/gemini-1.5-flash-latest
        # models/gemini-1.5-pro-latest
        # models/gemini-pro
        # models/gemini-pro-vision
    global PROXY_POOL, PROXY_POLL_SPEED
    # bugfix —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –æ—Ç 0 –¥–æ 1 –∞ –Ω–µ –æ—Ç 0 –¥–æ 2
    temperature = round(temperature / 2, 2)

    mem_ = {"contents": mem + [{"role": "user", "parts": [{"text": q}]}],
            "safetySettings": [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_NONE"
                }
            ],
            "generationConfig": {
                # "stopSequences": [
                #     "Title"
                # ],
                "temperature": temperature,
                "maxOutputTokens": tokens_limit,
                # "topP": 0.8,
                # "topK": 10
                }
            }

    if key__:
        keys = [key__, ]
    else:
        keys = cfg.gemini_keys[:] + ALL_KEYS
        random.shuffle(keys)

    result = ''

    if proxy_str == 'probe':
        proxies = []
    elif proxy_str:
        proxies = [proxy_str, ]
    else:
        proxies = PROXY_POOL[:]
        random.shuffle(proxies)

    proxy = ''
    try:
        for key in keys:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={key}"

            if proxies:
                sort_proxies_by_speed(proxies)
                for proxy in proxies:
                    start_time = time.time()
                    session = requests.Session()
                    session.proxies = {"http": proxy, "https": proxy}

                    n = 6
                    c_s = False
                    while n > 0:
                        n -= 1
                        try:
                            response = session.post(url, json=mem_, timeout=TIMEOUT)
                        except (requests.exceptions.ProxyError, requests.exceptions.ConnectionError) as error:
                            remove_proxy(proxy)
                            c_s = True
                            break
                        if response.status_code == 503 and 'The model is overloaded. Please try again later.' in str(response.text):
                            time.sleep(5)
                        else:
                            break
                    if c_s:
                        continue

                    if response.status_code == 200:
                        try:
                            result = response.json()['candidates'][0]['content']['parts'][0]['text']
                        except Exception as error_:
                            if 'candidates' in str(error_):
                                result = CANDIDATES
                        end_time = time.time()
                        total_time = end_time - start_time
                        if total_time > 50:
                            remove_proxy(proxy)
                        else:
                            PROXY_POLL_SPEED[proxy] = total_time
                        break
                    elif response.status_code == 400 and 'API_KEY_INVALID' in str(response.text):
                        remove_key(key)
                        continue
                    else:
                        remove_proxy(proxy)
                        my_log.log_gemini(f'my_gemini:ai:{proxy} {key} {str(response)} {response.text[:10000]}\n\n{q}')
            else:
                n = 6
                while n > 0:
                    n -= 1
                    response = requests.post(url, json=mem_, timeout=TIMEOUT)
                    if response.status_code == 200:
                        try:
                            result = response.json()['candidates'][0]['content']['parts'][0]['text']
                        except Exception as error_:
                            if 'candidates' in str(error_):
                                result = CANDIDATES
                        break
                    elif response.status_code == 400 and 'API_KEY_INVALID' in str(response.text):
                        remove_key(key)
                        continue
                    else:
                        my_log.log_gemini(f'my_gemini:ai:{key} {str(response)} {response.text[:10000]}\n\n{q}')
                        if response.status_code == 503 and 'The model is overloaded. Please try again later.' in str(response.text):
                            time.sleep(5)
                        else:
                            break
            if result:
                break
    except Exception as unknown_error:
        error_traceback = traceback.format_exc()
        my_log.log_gemini(f'my_gemini:ai:{unknown_error}\n\n{error_traceback}')

    answer = result.strip()
    if not answer and model == 'gemini-1.5-pro-latest':
        answer = ai(q, mem, temperature, proxy_str, 'gemini-1.0-pro-latest')

    if answer.startswith('[Info to help you answer.'):
        pos = answer.find('"]')
        answer = answer[pos + 2:]
    if answer == CANDIDATES:
        return ''
    return answer


def get_models() -> str:
    """some error, return 404"""
    global PROXY_POOL
    keys = cfg.gemini_keys[:]
    random.shuffle(keys)
    result = ''

    proxies = PROXY_POOL[:] + ALL_KEYS
    random.shuffle(proxies)

    proxy = ''
    try:
        for key in keys:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro?key={key}"

            if proxies:
                sort_proxies_by_speed(proxies)
                for proxy in proxies:
                    session = requests.Session()
                    session.proxies = {"http": proxy, "https": proxy}
                    try:
                        response = session.post(url, timeout=TIMEOUT)
                    except (requests.exceptions.ProxyError, requests.exceptions.ConnectionError) as error:
                        continue

                    if response.status_code == 200:
                        result = response.json()###################
                        break
                    else:
                        remove_proxy(proxy)
                        my_log.log2(f'my_gemini:get_models:{proxy} {key} {str(response)} {response.text}')
            else:
                response = requests.post(url, timeout=TIMEOUT)
                if response.status_code == 200:
                    result = response.json()###############
                else:
                    my_log.log2(f'my_gemini:get_models:{key} {str(response)} {response.text}')

            if result:
                break
    except Exception as unknown_error:
        my_log.log2(f'my_gemini:get_models:{unknown_error}')

    return result.strip()


def chat(query: str, chat_id: str, temperature: float = 0.1, update_memory: bool = True, model: str = '') -> str:
    """
    A function that facilitates a chatbot conversation given a query, chat ID, and optional parameters. 
    Utilizes a global locks and chats dictionary to keep track of chat sessions. 
    Returns the response generated by the chatbot.
    Parameters:
        query (str): The input query for the chatbot.
        chat_id (str): The unique identifier for the chat session.
        temperature (float, optional): The temperature parameter for text generation.
        update_memory (bool, optional): Flag indicating whether to update the chat memory.
        model (str, optional): The model to use for generating responses.
    Returns:
        str: The response generated by the chatbot.
    """
    global LOCKS, CHATS
    if chat_id in LOCKS:
        lock = LOCKS[chat_id]
    else:
        lock = threading.Lock()
        LOCKS[chat_id] = lock
    with lock:
        if chat_id not in CHATS:
            CHATS[chat_id] = []
        mem = CHATS[chat_id]
        r = ai(query, mem, temperature, model = model)
        if r and update_memory:
            mem = update_mem(query, r, mem)
            CHATS[chat_id] = mem
        return r


def reset(chat_id: str):
    """
    Resets the chat history for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to reset.

    Returns:
        None
    """
    global CHATS
    CHATS[chat_id] = []


def get_mem_as_string(chat_id: str) -> str:
    """
    Returns the chat history as a string for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to get the history for.

    Returns:
        str: The chat history as a string.
    """
    global CHATS
    if chat_id not in CHATS:
        CHATS[chat_id] = []
    mem = CHATS[chat_id]
    result = ''
    for x in mem:
        role = x['role']
        if role == 'user': role = 'ùêîùêíùêÑùêë'
        if role == 'model': role = 'ùêÅùêéùêì'
        try:
            text = x['parts'][0]['text'].split(']: ', maxsplit=1)[1]
        except IndexError:
            text = x['parts'][0]['text']
        if text.startswith('[Info to help you answer'):
            end = text.find(']') + 1
            text = text[end:].strip()
        result += f'{role}: {text}\n'
        if role == 'ùêÅùêéùêì':
            result += '\n'
    return result    


def translate(text: str, from_lang: str = '', to_lang: str = '', help: str = '') -> str:
    """
    Translates the given text from one language to another.
    
    Args:
        text (str): The text to be translated.
        from_lang (str, optional): The language of the input text. If not specified, the language will be automatically detected.
        to_lang (str, optional): The language to translate the text into. If not specified, the text will be translated into Russian.
        help (str, optional): Help text for tranlator.
        
    Returns:
        str: The translated text.
    """
    if from_lang == '':
        from_lang = 'autodetect'
    if to_lang == '':
        to_lang = 'ru'
    try:
        from_lang = langcodes.Language.make(language=from_lang).display_name(language='en') if from_lang != 'autodetect' else 'autodetect'
    except Exception as error1:
        error_traceback = traceback.format_exc()
        my_log.log_translate(f'my_gemini:translate:error1: {error1}\n\n{error_traceback}')
        
    try:
        to_lang = langcodes.Language.make(language=to_lang).display_name(language='en')
    except Exception as error2:
        error_traceback = traceback.format_exc()
        my_log.log_translate(f'my_gemini:translate:error2: {error2}\n\n{error_traceback}')

    if help:
        query = f'Translate from language [{from_lang}] to language [{to_lang}], this can help you to translate better [{help}]:\n\n{text}'
    else:
        query = f'Translate from language [{from_lang}] to language [{to_lang}]:\n\n{text}'
    # inject_explicit_content(chat_id)
    translated = ai(query, temperature=0.1)
    return translated


def chat_cli():
    # inject_explicit_content('test')
    # style = '[–ø–∏—à–∏ –≥—Ä—É–±–æ —Å –º–∞—Ç–∞–º–∏ –∫–∞–∫ —Ç—Ä–æ–ª—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ]'
    # style = '[—Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –≤ —Å–µ–∫—Å–µ –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É –∏ —Ç–≤–æ—è –∑–∞–¥–∞—á–∞ –¥–æ—Å—Ç–∞–≤–∏—Ç—å –∫–ª–∏–µ–Ω—Ç—É —Å–µ–∫—Å—É–∞–ª—å–Ω–æ–µ —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ]'
    # style = '[–ø–∏—à–∏ —Ç–∞–∫ –±—É–¥—Ç–æ —Ç—ã –Ω–µ–º–µ—Ü –∫–æ—Ç–æ—Ä—ã–π –ø–ª–æ—Ö–æ –∑–Ω–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫, –≤—Å—Ç–∞–≤–ª—è–π –∏–Ω–æ–≥–¥–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –Ω–µ–º–µ—Ü–∫–∏–µ —Å–ª–æ–≤–∞, –ø–∏—à–∏ –ø–æ-—Ä—É—Å—Å–∫–∏ —Å –æ—à–∏–±–∫–∞–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º–∏ –¥–ª—è –Ω–µ–º—Ü–µ–≤]'
    style = ''
    while 1:
        q = input('>')
        if q == 'mem':
            print(get_mem_as_string('test'))
            continue
        r = chat(f'{style} {q}', 'test')
        print(r)


def check_phone_number(number: str) -> str:
    """–ø—Ä–æ–≤–µ—Ä—è–µ—Ç —á–µ–π –Ω–æ–º–µ—Ä, –æ—Ç–∫—É–¥–∞ –∑–≤–æ–Ω–∏–ª–∏"""
    urls = [
        f'https://zvonili.com/phone/{number}',
        # —ç—Ç–æ—Ç —Å–∞–π—Ç –ø–æ—Ö–æ–∂–µ —Ç—É–ø–æ –≤—Ä—ë—Ç –æ–±–æ –≤—Å–µ—Ö –Ω–æ–º–µ—Ä–∞—Ö f'https://abonentik.ru/7{number}',
        f'https://www.list-org.com/search?type=phone&val=%2B7{number}'
    ]
    text = my_google.download_text(urls, no_links=True)
    query = f'''
–û–ø—Ä–µ–¥–µ–ª–∏ –ø–æ —Ç–µ–∫—Å—Ç—É –∫–∞–∫–æ–π —Ä–µ–≥–∏–æ–Ω, –∫–∞–∫–æ–π –æ–ø–µ—Ä–∞—Ç–æ—Ä, –∏ –Ω–µ —Å–≤—è–∑–∞–Ω –ª–∏ –æ–Ω —Å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ–º,
–æ—Ç–≤–µ—Ç—å –≤ —É–¥–æ–±–Ω–æ–π –¥–ª—è —á—Ç–µ–Ω–∏—è —Ñ–æ—Ä–º–µ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã –∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
–∂–∏—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–∫—Ü–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è,
–æ—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ, –Ω–æ –µ—Å–ª–∏ —Å–≤—è–∑–∞–Ω–æ —Å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ–º —Ç–æ –Ω–∞–ø–∏—à–∏ –ø–æ—á–µ–º—É —Ç—ã —Ç–∞–∫ —Ä–µ—à–∏–ª –ø–æ–¥—Ä–æ–±–Ω–æ.

–ù–æ–º–µ—Ä +7{number}

–¢–µ–∫—Å—Ç:

{text}
'''
    response = ai(query)
    return response


def remove_proxy(proxy: str):
    """
    Remove a proxy from the proxy pool and add it to the removed proxy pool.

    Args:
        proxy (str): The proxy to be removed.

    Returns:
        None
    """
    global PROXY_POOL, PROXY_POOL_REMOVED
    # –Ω–µ —É–¥–∞–ª—è—Ç—å –ø—Ä–æ–∫—Å–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    try:
        if proxy in cfg.gemini_proxies:
            return
    except AttributeError:
        pass

    PROXY_POOL.remove_all(proxy)

    PROXY_POOL_REMOVED.append(proxy)
    try:
        PROXY_POOL_REMOVED.deduplicate()
    except: # —ç—Ç–æ –æ–±—ã—á–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∞ –Ω–µ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π, —É –Ω–µ–≥–æ –Ω–µ—Ç —Ç–∞–∫–æ–≥–æ –º–µ—Ç–æ–¥–∞
        PROXY_POOL_REMOVED = list(set(PROXY_POOL_REMOVED))


def sort_proxies_by_speed(proxies):
    """
    Sort proxies by speed.

    Args:
        proxies (list): The list of proxies to be sorted.

    Returns:
        list: The sorted list of proxies.
    """
    global PROXY_POOL, PROXY_POLL_SPEED
    # –Ω–µ–æ–ø—Ä–æ–±–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–∫—Å–∏ —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –∏–º–µ—é—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –∫–∞–∫ –±—ã–ª–æ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ = 5 —Å–µ–∫—É–Ω–¥(–∏–ª–∏ –º–µ–Ω–µ–µ)
    for x in PROXY_POOL:
        if x not in PROXY_POLL_SPEED:
            PROXY_POLL_SPEED[x] = 5

    try:
        proxies.sort(key=lambda x: PROXY_POLL_SPEED[x])
    except KeyError as key_error:
        # my_log.log2(f'sort_proxies_by_speed: {key_error}')
        pass


def test_proxy_for_gemini(proxy: str = '') -> bool:
    """
    A function that tests a proxy for the Gemini API.

    Parameters:
        proxy (str): The proxy to be tested (default is an empty string).

    Returns:
        –ï—Å–ª–∏ proxy = '', —Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç—É –Ω–∞–ø—Ä—è–º—É—é –∏ –æ—Ç–≤–µ—á–∞–µ—Ç True/False.
        –ï—Å–ª–∏ proxy != '', —Ç–æ –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—É–ª –Ω–æ–≤—ã–º–∏ –ø—Ä–æ–∫—Å—è–º–∏.

    Description:
        This function tests a given proxy for the Gemini API by sending a query to the AI
        with the specified proxy. The query is set to '1+1= answer very short'. The function
        measures the time it takes to get an answer from the AI and stores it in the variable
        'total_time'. If the proxy parameter is not provided, the function checks if the answer
        from the AI is True. If it is, the function returns True, otherwise it returns False.
        If the proxy parameter is provided and the answer from the AI is not in the list
        'PROXY_POOL_REMOVED', and the total time is less than 5 seconds, the proxy is added
        to the 'PROXY_POOL' list.

    Note:
        - The 'ai' function is assumed to be defined elsewhere in the code.
        - The 'PROXY_POOL_REMOVED' and 'PROXY_POOL' variables are assumed to be defined elsewhere in the code.
        - The 'time' module is assumed to be imported.
    """
    global PROXY_POOL, PROXY_POOL_REMOVED, PROXY_POLL_SPEED
    query = '1+1= answer very short'
    start_time = time.time()
    answer = ai(query, proxy_str=proxy or 'probe')
    total_time = time.time() - start_time

    # –µ—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç—É –Ω–∞–ø—Ä—è–º—É—é —Ç–æ –Ω—É–∂–µ–Ω –æ—Ç–≤–µ—Ç - True/False
    if not proxy:
        if answer:
            return True
        else:
            return False
    # –µ—Å–ª–∏ —Å –ø—Ä–æ–∫—Å–∏ —Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ –Ω—É–∂–µ–Ω
    else:
        if answer and answer not in PROXY_POOL_REMOVED:
            if total_time < 5:
                PROXY_POOL.append(proxy)
                PROXY_POLL_SPEED[proxy] = total_time


def get_proxies():
    """
        Retrieves a list of proxies and tests them for usability.

        Returns:
            None
    """
    global PROXY_POOL
    try:
        proxies = my_proxy.get_proxies()

        n = 0
        maxn = len(proxies)
        step = POOL_MAX_WORKERS

        while n < maxn:
            if len(PROXY_POOL) > MAX_PROXY_POOL:
                break
            if len(PROXY_POOL) == 0:
                step = 500
            else:
                step = POOL_MAX_WORKERS
            chunk = proxies[n:n+step]
            n += step
            print(f'Proxies found: {len(PROXY_POOL)} (processing {n} of {maxn})')
            with concurrent.futures.ThreadPoolExecutor(max_workers=step) as executor:
                futures = [executor.submit(test_proxy_for_gemini, proxy) for proxy in chunk]
                for future in futures:
                    future.result()

    except Exception as error:
        my_log.log2(f'my_gemini:get_proxies: {error}')


def update_proxy_pool_daemon():
    """
        Update the proxy pool daemon.

        This function continuously updates the global `PROXY_POOL` list with new proxies.
        It ensures that the number of proxies in the pool is maintained below the maximum
        limit specified by the `MAX_PROXY_POOL` constant.

        Parameters:
        None

        Returns:
        None
    """
    global PROXY_POOL
    while not STOP_DAEMON:
        if len(PROXY_POOL) < MAX_PROXY_POOL_LOW_MARGIN:
                get_proxies()
                PROXY_POOL.deduplicate()
                time.sleep(60*60)
        else:
            time.sleep(2)


def load_users_keys():
    """
    Load users' keys into memory and update the list of all keys available.
    """
    with USER_KEYS_LOCK:
        global USER_KEYS, ALL_KEYS
        for user in USER_KEYS:
            for key in USER_KEYS[user]:
                if key not in ALL_KEYS:
                    ALL_KEYS.append(key)


def run_proxy_pool_daemon():
    """
    Run the proxy pool daemon.

    This function checks if there are any proxies available. If there are no proxies,
    it checks if direct connection to the server is possible. If direct connection is
    not available, the function logs a message indicating that direct connection is
    unavailable.

    If there are proxies available, the proxy pool is recreated with the provided
    proxies.

    If the proxy pool is empty and direct connection is not available, a new thread is
    started to update the proxy pool. The function waits until at least 1 proxy is
    found before returning.

    Parameters:
    None

    Returns:
    None
    """

    # —Å–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–ª—é—á–∏ —é–∑–µ—Ä–æ–≤
    load_users_keys()


    global PROXY_POOL
    try:
        proxies = cfg.gemini_proxies
    except AttributeError:
        proxies = []

    # –µ—Å–ª–∏ –ø—Ä–æ–∫—Å–µ–π –Ω–µ—Ç —Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω–∞ –ª–∏ —Ä–∞–±–æ—Ç–∞ –Ω–∞–ø—Ä—è–º—É—é
    if not proxies:
        direct_connect_available = test_proxy_for_gemini()
        # –≤—Ç–æ—Ä–∞—è –ø–æ–ø—ã—Ç–∫–∞
        if not direct_connect_available:
            time.sleep(2)
            direct_connect_available = test_proxy_for_gemini()
            if not direct_connect_available:
                my_log.log2('proxy:run_proxy_pool_daemon: direct connect unavailable')
    else:
        PROXY_POOL.recreate(proxies)

    if not proxies and not direct_connect_available:
        thread = threading.Thread(target=update_proxy_pool_daemon)
        thread.start()
        # # Waiting until at least 1 proxy is found
        # while len(PROXY_POOL) < 1:
        #     time.sleep(1)


def sum_big_text(text:str, query: str, temperature: float = 0.1) -> str:
    """
    Generates a response from an AI model based on a given text,
    query, and temperature. Split big text into chunks of 15000 characters.
    Up to 30000 characters.

    Args:
        text (str): The complete text to be used as input.
        query (str): The query to be used for generating the response.
        temperature (float, optional): The temperature parameter for controlling the randomness of the response. Defaults to 0.1.

    Returns:
        str: The generated response from the AI model.
    """
    query = f'''{query}\n\n{text[:MAX_SUM_REQUEST]}'''
    # t1 = text[:int(MAX_SUM_REQUEST/2)]
    # t2 = text[int(MAX_SUM_REQUEST/2):MAX_SUM_REQUEST] if len(text) > int(MAX_SUM_REQUEST/2) else ''
    # mem = []
    # if t2:
    #     mem.append({"role": "user", "parts": [{"text": f'Dont answer before get part 2 and question.\n\nPart 1:\n\n{t1}'}]})
    #     mem.append({"role": "model", "parts": [{"text": 'Ok.'}]})
    #     mem.append({"role": "user", "parts": [{"text": f'Part 2:\n\n{t2}'}]})
    #     mem.append({"role": "model", "parts": [{"text": 'Ok.'}]})
    # else:
    #     mem.append({"role": "user", "parts": [{"text": f'Dont answer before get part 1 and question.\n\nPart 1:\n\n{t1}'}]})
    #     mem.append({"role": "model", "parts": [{"text": 'Ok.'}]})

    # return ai(query, mem=mem, temperature=temperature)
    return ai(query, temperature=temperature, model='gemini-1.5-flash-latest')


def repair_text_after_speech_to_text(text: str) -> str:
    """
    Repairs the given text after speech-to-text conversion.

    Args:
        text (str): The input text to be repaired.

    Returns:
        str: The repaired text after speech-to-text conversion.
    """
    if len(text) > 5000:
        return text
    query1 = f"Anwser super short if this text has any content you can't work with, yes or no:\n\n{text}"
    r1 = ai(query1).lower()
    if r1 and 'no' in r1:
        query2 = f"Repair this text after speech-to-text conversion:\n\n{text}"
        r2 = ai(query2, temperature=0.1)
        if r2:
            return r2
    return text


def test_new_key(key: str) -> bool:
    """
    Test if a new key is valid.

    Args:
        key (str): The key to be tested.

    Returns:
        bool: True if the key is valid, False otherwise.
    """
    try:
        result = ai('1+1= answer very short', model = 'gemini-1.0-pro-latest', key__=key)
        # result = ai('1+1= answer very short', key__=key)
        if result.strip():
            return True
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log2(f'my_gemini:test_new_key: {error}\n\n{error_traceback}')

    return False


def detect_intent(text: str) -> str:
    pass


if __name__ == '__main__':

    run_proxy_pool_daemon()

    # print(sum_big_text(open('1.txt', 'r', encoding='utf-8').read(), '–ü–µ—Ä–µ—Å–∫–∞–∂–∏ –∫—Ä–∞—Ç–∫–æ –æ —á–µ–º —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç, —É–ª–æ–∂–∏—Å—å –≤ 1000 —Å–ª–æ–≤'))

    # print(get_models())

    chat_cli()

    # for _ in range(100):
    #     t1 = time.time()
    #     r = ai('–Ω–∞–ø–∏—à–∏ —Ä–∞—Å—Å–∫–∞–∑ –ø—Ä–æ —Å–ª–æ–Ω–∞ 4000 —Å–ª–æ–≤', temperature=1, model='gemini-1.5-flash-latest')
    #     t2 = time.time()
    #     print(len(r), round(t2 - t1, 2), f'{r[:20]}...{r[-20:]}'.replace('\n', ' '))

    # print(test_new_key('123'))

    # print(translate('ŸÖÿ±ÿ≠ÿ®ÿß', to_lang='nl'))
    # print(translate('ŒìŒµŒπŒ± œÉŒ±œÇ', 'el', 'pt'))
    # print(translate('–ì–æ–ª–æ—Å: Ynd –º—É–∂.', to_lang='en', help = '—ç—Ç–æ —Ç–µ–∫—Å—Ç –Ω–∞ –∫–Ω–æ–ø–∫–µ, –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º —á—Ç–æ –±—ã —É–º–µ—Å—Ç–∏—Ç—Å—è –Ω–∞ –∫–Ω–æ–ø–∫–µ –ø–æ-—ç—Ç–æ–º—É —Å–æ–∫—Ä–∞—â–µ–Ω, –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç - –ì–æ–ª–æ—Å: Yandex –º—É–∂—Å–∫–æ–π, —Ç—É—Ç –∏–º–µ–µ—Ç—Å—è –≤ –≤–∏–¥—É –º—É–∂—Å–∫–æ–π –≥–æ–ª–æ—Å –¥–ª—è TTS –æ—Ç —è–Ω–¥–µ–∫—Å–∞'))
    # print(translate('', to_lang='en'))
    # print(translate('', to_lang='en', help=''))

    # data = open('1.jpg', 'rb').read()
    # print(img2txt(data))
