#!/usr/bin/env python3
# https://ai.google.dev/
# pip install langcodes[data]


import base64
import random
import threading
import time
import requests
import traceback

import langcodes
from sqlitedict import SqliteDict

import cfg
import my_google
import my_log


STOP_DAEMON = False


# ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÑŽÐ·ÐµÑ€ Ð´Ð°ÐµÑ‚ ÑÐ²Ð¾Ð¸ ÐºÐ»ÑŽÑ‡Ð¸ Ð¸ Ð¾Ð½Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð½Ð¾ ÑÐ¾ Ð²ÑÐµÐ¼Ð¸
# ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡ Ð´Ð°ÐµÑ‚ Ð²ÑÐµÐ³Ð¾ 50 Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð² Ð´ÐµÐ½ÑŒ Ñ‚Ð°Ðº Ñ‡Ñ‚Ð¾ Ñ‡ÐµÐ¼ Ð±Ð¾Ð»ÑŒÑˆÐµ Ñ‚ÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐµ
# Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ - 32Ðº Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð² Ð¼Ð¸Ð½ÑƒÑ‚Ñƒ, 2 Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð² Ð¼Ð¸Ð½ÑƒÑ‚Ñƒ
# {full_chat_id as str: list of keys as list of str}
# {'[2654672534] [0]': ['key1','key2','key3'], ...}
USER_KEYS = SqliteDict('db/gemini_user_keys.db', autocommit=True)
# list of all users keys
ALL_KEYS = []
USER_KEYS_LOCK = threading.Lock()


# Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ Ð´Ð»Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ðº gemini
TIMEOUT = 120


# Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²ÐºÐ° Ñ‡Ð°Ñ‚Ð¾Ð² Ñ‡Ñ‚Ð¾ Ð±Ñ‹ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ 
# {id:lock}
LOCKS = {}

# Ð½Ðµ Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð±Ð¾Ð»ÑŒÑˆÐµ Ñ‡ÐµÐ¼, ÑÑ‚Ð¾ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ñ‚ÐµÐ»ÐµÐ³Ñ€Ð°Ð¼ Ð±Ð¾Ñ‚Ð°, Ð² ÑÑ‚Ð¾Ð¼ Ð¼Ð¾Ð´ÑƒÐ»Ðµ Ð¾Ð½Ð¾ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ
MAX_REQUEST = 25000

# Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ (32Ðº Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Google?)
# MAX_CHAT_SIZE = 25000
MAX_CHAT_SIZE = 31000
# ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¿Ð¾Ð¼Ð½Ð¸Ñ‚ÑŒ, Ð´Ð»Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
MAX_CHAT_LINES = 40
if hasattr(cfg, 'GEMINI_MAX_CHAT_LINES'):
    MAX_CHAT_LINES = cfg.GEMINI_MAX_CHAT_LINES

# Ð¼Ð¾Ð¶Ð½Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ 2 Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð¿Ð¾ 15000 Ð² ÑÑƒÐ¼Ð¼Ðµ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑÑ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð¾Ð¼ 30000
# Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾ Ð´Ð»Ñ ÑÑƒÐ¼Ð¼Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²
MAX_SUM_REQUEST = 200000
# MAX_SUM_REQUEST = 31000

# Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð² {id:list(mem)}
CHATS = SqliteDict('db/gemini_dialogs.db', autocommit=True)

# magic string
CANDIDATES = '78fgh892890df@d7gkln2937DHf98723Dgh'


def img2txt(data_: bytes, prompt: str = "Ð§Ñ‚Ð¾ Ð½Ð° ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐµ, Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ð¾?") -> str:
    """
    Generates a textual description of an image based on its contents.

    Args:
        data_: The image data as bytes.
        prompt: The prompt to provide for generating the description. Defaults to "Ð§Ñ‚Ð¾ Ð½Ð° ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐµ, Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ð¾?".

    Returns:
        A textual description of the image.

    Raises:
        None.
    """
    try:
        img_data = base64.b64encode(data_).decode("utf-8")
        data = {
            "contents": [
                {
                "parts": [
                    {"text": prompt},
                    {
                    "inline_data": {
                        "mime_type": "image/jpeg",
                        "data": img_data
                    }
                    }
                ]
                }
            ]
            }

        result = ''
        keys = cfg.gemini_keys[:]  + ALL_KEYS
        random.shuffle(keys)
        keys = keys[:4]

        proxies = cfg.gemini_proxies if hasattr(cfg, 'gemini_proxies') else None
        if proxies:
            random.shuffle(proxies)

        for api_key in keys:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro-vision:generateContent?key={api_key}"

            if proxies:
                for proxy in proxies:
                    session = requests.Session()
                    session.proxies = {"http": proxy, "https": proxy}
                    try:
                        response = session.post(url, json=data, timeout=TIMEOUT).json()
                        try:
                            result = response['candidates'][0]['content']['parts'][0]['text']
                            if result == '' or result:
                                return result.strip()
                        except Exception as error_ca:
                            if 'candidates' not in str(error_ca) and 'content' in str(error_ca):
                                my_log.log2(f'my_gemini:img2txt:{error_ca}')
                                return ''
                        if result:
                            break
                        if result == '':
                            break
                    except (requests.exceptions.ProxyError, requests.exceptions.ConnectionError) as error:
                        continue
            else:
                try:
                    response = requests.post(url, json=data, timeout=TIMEOUT).json()
                    try:
                        result = response['candidates'][0]['content']['parts'][0]['text']
                        if result == '' or result:
                            return result.strip()
                    except Exception as error_ca:
                        if 'candidates' not in str(error_ca) and 'content' in str(error_ca):
                            my_log.log2(f'my_gemini:img2txt:{error_ca}')
                            return ''
                except Exception as error:
                    if 'content' in str(error):
                        return ''
                    my_log.log2(f'img2txt:{error}')
        return result.strip()
    except Exception as unknown_error:
        if 'content' not in str(unknown_error):
            my_log.log2(f'my_gemini:img2txt:{unknown_error}')
    return ''


def update_mem(query: str, resp: str, mem):
    """
    Update the memory with the given query and response.

    Parameters:
        query (str): The input query.
        resp (str): The response to the query.
        mem: The memory object to update, if str than mem is a chat_id

    Returns:
        list: The updated memory object.
    """
    global CHATS
    chat_id = ''
    if isinstance(mem, str): # if mem - chat_id
        chat_id = mem
        if mem not in CHATS:
            CHATS[mem] = []
        mem = CHATS[mem]

    mem.append({"role": "user", "parts": [{"text": query}]})
    mem.append({"role": "model", "parts": [{"text": resp}]})
    size = 0
    for x in mem:
        text = x['parts'][0]['text']
        size += len(text)
    while size > MAX_CHAT_SIZE:
        mem = mem[2:]
        size = 0
        for x in mem:
            text = x['parts'][0]['text']
            size += len(text)
    mem = mem[-MAX_CHAT_LINES*2:]
    if chat_id:
        CHATS[chat_id] = mem
    return mem


def undo(chat_id: str):
    """
    Undo the last two lines of chat history for a given chat ID.

    Args:
        chat_id (str): The ID of the chat.

    Raises:
        Exception: If there is an error while undoing the chat history.

    Returns:
        None
    """
    try:
        global LOCKS, CHATS

        if chat_id in LOCKS:
            lock = LOCKS[chat_id]
        else:
            lock = threading.Lock()
            LOCKS[chat_id] = lock
        with lock:
            if chat_id in CHATS:
                mem = CHATS[chat_id]
                # remove 2 last lines from mem
                mem = mem[:-2]
                CHATS[chat_id] = mem
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_gemini(f'Failed to undo chat {chat_id}: {error}\n\n{error_traceback}')


def remove_key(key: str):
    """
    Removes a given key from the ALL_KEYS list and from the USER_KEYS dictionary.
    
    Args:
        key (str): The key to be removed.
        
    Returns:
        None
    """
    try:
        if key in ALL_KEYS:
            del ALL_KEYS[ALL_KEYS.index(key)]
        with USER_KEYS_LOCK:
            # remove key from USER_KEYS
            for user in USER_KEYS:
                if key in USER_KEYS[user]:
                    USER_KEYS[user] = [x for x in USER_KEYS[user] if x != key]
                    my_log.log_gemini(f'Invalid key {key} removed from user {user}')
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_gemini(f'Failed to remove key {key}: {error}\n\n{error_traceback}')


def ai(q: str, mem = [],
       temperature: float = 0.1,
       proxy_str: str = '',
       model: str = '',
       key__: str = None,
       tokens_limit: int = 8000) -> str:
    """
    Generates a response to a given question using the Generative AI model.
    
    Args:
        q (str): The question to be answered.
        mem (list, optional): The memory to be used for generating the response. Defaults to [].
        temperature (float, optional): The temperature parameter for the model. Defaults to 0.1.
        proxy_str (str, optional): The proxy to be used for the request. Defaults to ''.
        model (str, optional): The model to be used for generating the response. Defaults to ''.
        key__ (str, optional): The API key to be used for the request. Defaults to None.
        
    Returns:
        str: The generated response to the question.
        
    Raises:
        Exception: If an error occurs during the request or response handling.
    """
    if model == '':
        model = 'gemini-1.5-flash-latest'
        # models/gemini-1.0-pro
        # models/gemini-1.0-pro-001
        # models/gemini-1.0-pro-latest
        # models/gemini-1.0-pro-vision-latest
        # models/gemini-1.5-flash-latest
        # models/gemini-1.5-pro-latest
        # models/gemini-pro
        # models/gemini-pro-vision

    # bugfix Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð½Ð° ÑÐ°Ð¼Ð¾Ð¼ Ð´ÐµÐ»Ðµ Ð¾Ñ‚ 0 Ð´Ð¾ 1 Ð° Ð½Ðµ Ð¾Ñ‚ 0 Ð´Ð¾ 2
    temperature = round(temperature / 2, 2)

    mem_ = {"contents": mem + [{"role": "user", "parts": [{"text": q}]}],
            "safetySettings": [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_NONE"
                }
            ],
            "generationConfig": {
                # "stopSequences": [
                #     "Title"
                # ],
                "temperature": temperature,
                "maxOutputTokens": tokens_limit,
                # "topP": 0.8,
                # "topK": 10
                }
            }

    if key__:
        keys = [key__, ]
    else:
        keys = cfg.gemini_keys[:] + ALL_KEYS
        random.shuffle(keys)
        keys = keys[:4]

    result = ''

    if proxy_str == 'probe':
        proxies = []
    elif proxy_str:
        proxies = [proxy_str, ]
    else:
        proxies = cfg.gemini_proxies if hasattr(cfg, 'gemini_proxies') else None
        if proxies:
            random.shuffle(proxies)

    proxy = ''
    try:
        for key in keys:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={key}"

            if proxies:
                for proxy in proxies:
                    session = requests.Session()
                    session.proxies = {"http": proxy, "https": proxy}

                    n = 6
                    c_s = False
                    while n > 0:
                        n -= 1
                        try:
                            response = session.post(url, json=mem_, timeout=TIMEOUT)
                        except (requests.exceptions.ProxyError, requests.exceptions.ConnectionError) as error:
                            c_s = True
                            break
                        if response.status_code == 503 and 'The model is overloaded. Please try again later.' in str(response.text):
                            time.sleep(5)
                        else:
                            break
                    if c_s:
                        continue

                    if response.status_code == 200:
                        try:
                            result = response.json()['candidates'][0]['content']['parts'][0]['text']
                        except Exception as error_:
                            if 'candidates' in str(error_):
                                result = CANDIDATES
                        break
                    elif response.status_code == 400 and 'API_KEY_INVALID' in str(response.text):
                        remove_key(key)
                        continue
                    else:
                        my_log.log_gemini(f'my_gemini:ai:{proxy} {key} {str(response)} {response.text[:1000]}\n\n{q}')
            else:
                n = 6
                while n > 0:
                    n -= 1
                    response = requests.post(url, json=mem_, timeout=TIMEOUT)
                    if response.status_code == 200:
                        try:
                            result = response.json()['candidates'][0]['content']['parts'][0]['text']
                        except Exception as error_:
                            if 'candidates' in str(error_):
                                result = CANDIDATES
                        break
                    elif response.status_code == 400 and 'API_KEY_INVALID' in str(response.text):
                        remove_key(key)
                        continue
                    else:
                        my_log.log_gemini(f'my_gemini:ai:{key} {str(response)} {response.text[:1000]}\n\n{q}')
                        if response.status_code == 503 and 'The model is overloaded. Please try again later.' in str(response.text):
                            time.sleep(5)
                        else:
                            break
            if result:
                break
    except Exception as unknown_error:
        error_traceback = traceback.format_exc()
        my_log.log_gemini(f'my_gemini:ai:{unknown_error}\n\n{error_traceback}')

    try:
        answer = result.strip()
    except:
        return ''

    if answer.startswith('[Info to help you answer.'):
        pos = answer.find('"]')
        answer = answer[pos + 2:]
    if answer == CANDIDATES:
        return ''

    return answer


def get_models() -> str:
    """some error, return 404"""
    global PROXY_POOL
    keys = cfg.gemini_keys[:]
    random.shuffle(keys)
    result = ''

    proxies = PROXY_POOL[:] + ALL_KEYS
    random.shuffle(proxies)

    proxy = ''
    try:
        for key in keys:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro?key={key}"

            if proxies:
                sort_proxies_by_speed(proxies)
                for proxy in proxies:
                    session = requests.Session()
                    session.proxies = {"http": proxy, "https": proxy}
                    try:
                        response = session.post(url, timeout=TIMEOUT)
                    except (requests.exceptions.ProxyError, requests.exceptions.ConnectionError) as error:
                        continue

                    if response.status_code == 200:
                        result = response.json()###################
                        break
                    else:
                        remove_proxy(proxy)
                        my_log.log2(f'my_gemini:get_models:{proxy} {key} {str(response)} {response.text}')
            else:
                response = requests.post(url, timeout=TIMEOUT)
                if response.status_code == 200:
                    result = response.json()###############
                else:
                    my_log.log2(f'my_gemini:get_models:{key} {str(response)} {response.text}')

            if result:
                break
    except Exception as unknown_error:
        my_log.log2(f'my_gemini:get_models:{unknown_error}')

    return result.strip()


def chat(query: str, chat_id: str, temperature: float = 0.1, update_memory: bool = True, model: str = '') -> str:
    """
    A function that facilitates a chatbot conversation given a query, chat ID, and optional parameters. 
    Utilizes a global locks and chats dictionary to keep track of chat sessions. 
    Returns the response generated by the chatbot.
    Parameters:
        query (str): The input query for the chatbot.
        chat_id (str): The unique identifier for the chat session.
        temperature (float, optional): The temperature parameter for text generation.
        update_memory (bool, optional): Flag indicating whether to update the chat memory.
        model (str, optional): The model to use for generating responses.
    Returns:
        str: The response generated by the chatbot.
    """
    global LOCKS, CHATS
    if chat_id in LOCKS:
        lock = LOCKS[chat_id]
    else:
        lock = threading.Lock()
        LOCKS[chat_id] = lock
    with lock:
        if chat_id not in CHATS:
            CHATS[chat_id] = []
        mem = CHATS[chat_id]
        r = ''
        try:
            r = ai(query, mem, temperature, model = model)
        except Exception as error:
            my_log.log_gemini(f'my_gemini:chat:{error}\n\n{query[:500]}')
            time.sleep(5)
            try:
                r = ai(query, mem, temperature, model = model)
            except Exception as error:
                my_log.log_gemini(f'my_gemini:chat:{error}\n\n{query[:500]}')
        if r and update_memory:
            mem = update_mem(query, r, mem)
            CHATS[chat_id] = mem
        return r


def reset(chat_id: str):
    """
    Resets the chat history for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to reset.

    Returns:
        None
    """
    global CHATS
    CHATS[chat_id] = []


def get_mem_for_llama(chat_id: str, l: int = 3):
    """
    Retrieves the recent chat history for a given chat_id. For using with llama.

    Parameters:
        chat_id (str): The unique identifier for the chat session.
        l (int, optional): The number of lines to retrieve. Defaults to 3.

    Returns:
        list: The recent chat history as a list of dictionaries with role and content.
    """
    global CHATS

    res_mem = []
    l = l*2

    if chat_id not in CHATS:
        CHATS[chat_id] = []
    mem = CHATS[chat_id]
    mem = mem[-l:]

    for x in mem:
        role = x['role']
        try:
            text = x['parts'][0]['text'].split(']: ', maxsplit=1)[1]
        except IndexError:
            text = x['parts'][0]['text']
        if role == 'user':
            res_mem += [{'role': 'user', 'content': text}]
        else:
            res_mem += [{'role': 'assistant', 'content': text}]

    return res_mem


def get_mem_as_string(chat_id: str) -> str:
    """
    Returns the chat history as a string for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to get the history for.

    Returns:
        str: The chat history as a string.
    """
    global CHATS
    if chat_id not in CHATS:
        CHATS[chat_id] = []
    mem = CHATS[chat_id]
    result = ''
    for x in mem:
        role = x['role']
        if role == 'user': role = 'ð”ð’ð„ð‘'
        if role == 'model': role = 'ððŽð“'
        try:
            text = x['parts'][0]['text'].split(']: ', maxsplit=1)[1]
        except IndexError:
            text = x['parts'][0]['text']
        if text.startswith('[Info to help you answer'):
            end = text.find(']') + 1
            text = text[end:].strip()
        result += f'{role}: {text}\n'
        if role == 'ððŽð“':
            result += '\n'
    return result    


def translate(text: str, from_lang: str = '', to_lang: str = '', help: str = '') -> str:
    """
    Translates the given text from one language to another.
    
    Args:
        text (str): The text to be translated.
        from_lang (str, optional): The language of the input text. If not specified, the language will be automatically detected.
        to_lang (str, optional): The language to translate the text into. If not specified, the text will be translated into Russian.
        help (str, optional): Help text for tranlator.
        
    Returns:
        str: The translated text.
    """
    if from_lang == '':
        from_lang = 'autodetect'
    if to_lang == '':
        to_lang = 'ru'
    try:
        from_lang = langcodes.Language.make(language=from_lang).display_name(language='en') if from_lang != 'autodetect' else 'autodetect'
    except Exception as error1:
        error_traceback = traceback.format_exc()
        my_log.log_translate(f'my_gemini:translate:error1: {error1}\n\n{error_traceback}')
        
    try:
        to_lang = langcodes.Language.make(language=to_lang).display_name(language='en')
    except Exception as error2:
        error_traceback = traceback.format_exc()
        my_log.log_translate(f'my_gemini:translate:error2: {error2}\n\n{error_traceback}')

    if help:
        query = f'Translate from language [{from_lang}] to language [{to_lang}], this can help you to translate better [{help}]:\n\n{text}'
    else:
        query = f'Translate from language [{from_lang}] to language [{to_lang}]:\n\n{text}'
    # inject_explicit_content(chat_id)
    translated = ai(query, temperature=0.1)
    return translated


def chat_cli():
    # inject_explicit_content('test')
    # style = '[Ð¿Ð¸ÑˆÐ¸ Ð³Ñ€ÑƒÐ±Ð¾ Ñ Ð¼Ð°Ñ‚Ð°Ð¼Ð¸ ÐºÐ°Ðº Ñ‚Ñ€Ð¾Ð»ÑŒ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ðµ]'
    # style = '[Ñ‚Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑˆÑŒ Ð² ÑÐµÐºÑÐµ Ð¿Ð¾ Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½Ñƒ Ð¸ Ñ‚Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ñƒ ÑÐµÐºÑÑƒÐ°Ð»ÑŒÐ½Ð¾Ðµ ÑƒÐ´Ð¾Ð²Ð¾Ð»ÑŒÑÑ‚Ð²Ð¸Ðµ]'
    # style = '[Ð¿Ð¸ÑˆÐ¸ Ñ‚Ð°Ðº Ð±ÑƒÐ´Ñ‚Ð¾ Ñ‚Ñ‹ Ð½ÐµÐ¼ÐµÑ† ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ð»Ð¾Ñ…Ð¾ Ð·Ð½Ð°ÐµÑ‚ Ñ€ÑƒÑÑÐºÐ¸Ð¹ ÑÐ·Ñ‹Ðº, Ð²ÑÑ‚Ð°Ð²Ð»ÑÐ¹ Ð¸Ð½Ð¾Ð³Ð´Ð° Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð½ÐµÐ¼ÐµÑ†ÐºÐ¸Ðµ ÑÐ»Ð¾Ð²Ð°, Ð¿Ð¸ÑˆÐ¸ Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸ Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ð¼Ð¸ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð½Ñ‹Ð¼Ð¸ Ð´Ð»Ñ Ð½ÐµÐ¼Ñ†ÐµÐ²]'
    style = ''
    while 1:
        q = input('>')
        if q == 'mem':
            print(get_mem_as_string('test'))
            continue
        r = chat(f'{style} {q}', 'test')
        print(r)


def check_phone_number(number: str) -> str:
    """Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ñ‡ÐµÐ¹ Ð½Ð¾Ð¼ÐµÑ€, Ð¾Ñ‚ÐºÑƒÐ´Ð° Ð·Ð²Ð¾Ð½Ð¸Ð»Ð¸"""
    urls = [
        f'https://zvonili.com/phone/{number}',
        # ÑÑ‚Ð¾Ñ‚ ÑÐ°Ð¹Ñ‚ Ð¿Ð¾Ñ…Ð¾Ð¶Ðµ Ñ‚ÑƒÐ¿Ð¾ Ð²Ñ€Ñ‘Ñ‚ Ð¾Ð±Ð¾ Ð²ÑÐµÑ… Ð½Ð¾Ð¼ÐµÑ€Ð°Ñ… f'https://abonentik.ru/7{number}',
        f'https://www.list-org.com/search?type=phone&val=%2B7{number}'
    ]
    text = my_google.download_text(urls, no_links=True)
    query = f'''
ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»Ð¸ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ ÐºÐ°ÐºÐ¾Ð¹ Ñ€ÐµÐ³Ð¸Ð¾Ð½, ÐºÐ°ÐºÐ¾Ð¹ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€, Ð¸ Ð½Ðµ ÑÐ²ÑÐ·Ð°Ð½ Ð»Ð¸ Ð¾Ð½ Ñ Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾Ð¼,
Ð¾Ñ‚Ð²ÐµÑ‚ÑŒ Ð² ÑƒÐ´Ð¾Ð±Ð½Ð¾Ð¹ Ð´Ð»Ñ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ñ„Ð¾Ñ€Ð¼Ðµ Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð° Ð°Ð±Ð·Ð°Ñ†Ñ‹ Ð¸ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼
Ð¶Ð¸Ñ€Ð½Ð¾Ð³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð° Ð´Ð»Ñ Ð°ÐºÑ†ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ,
Ð¾Ñ‚Ð²ÐµÑ‚ÑŒ ÐºÑ€Ð°Ñ‚ÐºÐ¾, Ð½Ð¾ ÐµÑÐ»Ð¸ ÑÐ²ÑÐ·Ð°Ð½Ð¾ Ñ Ð¼Ð¾ÑˆÐµÐ½Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾Ð¼ Ñ‚Ð¾ Ð½Ð°Ð¿Ð¸ÑˆÐ¸ Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ Ñ‚Ñ‹ Ñ‚Ð°Ðº Ñ€ÐµÑˆÐ¸Ð» Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ð¾.

ÐÐ¾Ð¼ÐµÑ€ +7{number}

Ð¢ÐµÐºÑÑ‚:

{text}
'''
    response = ai(query)
    return response, text


def load_users_keys():
    """
    Load users' keys into memory and update the list of all keys available.
    """
    with USER_KEYS_LOCK:
        global USER_KEYS, ALL_KEYS
        for user in USER_KEYS:
            for key in USER_KEYS[user]:
                if key not in ALL_KEYS:
                    ALL_KEYS.append(key)


def sum_big_text(text:str, query: str, temperature: float = 0.1) -> str:
    """
    Generates a response from an AI model based on a given text,
    query, and temperature. Split big text into chunks of 15000 characters.
    Up to 30000 characters.

    Args:
        text (str): The complete text to be used as input.
        query (str): The query to be used for generating the response.
        temperature (float, optional): The temperature parameter for controlling the randomness of the response. Defaults to 0.1.

    Returns:
        str: The generated response from the AI model.
    """
    query = f'''{query}\n\n{text[:MAX_SUM_REQUEST]}'''
    return ai(query, temperature=temperature, model='gemini-1.5-flash-latest')


def repair_text_after_speech_to_text(text: str) -> str:
    """
    Repairs the given text after speech-to-text conversion.

    Args:
        text (str): The input text to be repaired.

    Returns:
        str: The repaired text after speech-to-text conversion.
    """
    if len(text) > 5000:
        return text
    query1 = f"Anwser super short if this text has any content you can't work with, yes or no:\n\n{text}"
    r1 = ai(query1).lower()
    if r1 and 'no' in r1:
        query2 = f"Repair this text after speech-to-text conversion:\n\n{text}"
        r2 = ai(query2, temperature=0.1)
        if r2:
            return r2
    return text


def test_new_key(key: str) -> bool:
    """
    Test if a new key is valid.

    Args:
        key (str): The key to be tested.

    Returns:
        bool: True if the key is valid, False otherwise.
    """
    try:
        result = ai('1+1= answer very short', model = 'gemini-1.0-pro', key__=key)
        # result = ai('1+1= answer very short', key__=key)
        if result.strip():
            return True
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log2(f'my_gemini:test_new_key: {error}\n\n{error_traceback}')

    return False


def detect_intent(text: str) -> dict:
    """
    ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ ÐµÐ³Ð¾ Ð½Ð°Ð¼ÐµÑ€ÐµÐ½Ð¸Ðµ:
        - Ñ…Ð¾Ñ‡ÐµÑ‚ Ð»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ,
        - Ñ…Ð¾Ñ‡ÐµÑ‚ Ð»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚ Ð² Google,
        - Ñ…Ð¾Ñ‡ÐµÑ‚ Ð»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð·Ð°Ð´Ð°Ñ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ð¼Ñƒ ÑÑÑ‹Ð»ÐºÐ¸.

    Args:
        text (str): Ð–ÑƒÑ€Ð½Ð°Ð» Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÐºÐ¸ Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼.

    Returns:
        dict: Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ 'image', 'google', 'link',
              Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… (True/False) ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ Ð½Ð° Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ³Ð¾ Ð½Ð°Ð¼ÐµÑ€ÐµÐ½Ð¸Ñ.
    """
    result = {
        'image':    False, # ÑŽÐ·ÐµÑ€ Ñ…Ð¾Ñ‡ÐµÑ‚ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
        'google':   False, # ÑŽÐ·ÐµÑ€ Ñ…Ð¾Ñ‡ÐµÑ‚ Ð¸ÑÐºÐ°Ñ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚ Ð² Ð³ÑƒÐ³Ð»Ðµ
        'link':     False, # ÑŽÐ·ÐµÑ€ Ñ…Ð¾Ñ‡ÐµÑ‚ Ð·Ð°Ð´Ð°Ñ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ð¼Ñƒ ÑÑÑ‹Ð»ÐºÐ¸
              }

    query = f'''
ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»Ð¸ Ð¿Ð¾ Ð¶ÑƒÑ€Ð½Ð°Ð»Ñƒ Ñ‡Ð°Ñ‚Ð° ÐµÑÑ‚ÑŒ Ð»Ð¸ Ñƒ ÑŽÐ·ÐµÑ€Ð° Ð¶ÐµÐ»Ð°Ð½Ð¸Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½ Ð¸Ð· 3 ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÐµÐ²,
1. Ð®Ð·ÐµÑ€ Ñ…Ð¾Ñ‡ÐµÑ‚ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
2. Ð®Ð·ÐµÑ€ Ñ…Ð¾Ñ‡ÐµÑ‚ Ð¸ÑÐºÐ°Ñ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚ Ð² Ð³ÑƒÐ³Ð»Ðµ (Ð½Ð°Ð´Ð¾ Ð¿Ð¾Ð½ÑÑ‚ÑŒ Ð½ÑƒÐ¶Ð½Ð¾ Ð»Ð¸ Ð³ÑƒÐ³Ð»Ð¸Ñ‚ÑŒ Ñ‡Ñ‚Ð¾ Ð±Ñ‹ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ Ð½Ð° Ð·Ð°Ð¿Ñ€Ð¾Ñ ÑŽÐ·ÐµÑ€Ð°)
3. Ð®Ð·ÐµÑ€ Ñ…Ð¾Ñ‡ÐµÑ‚ Ð·Ð°Ð´Ð°Ñ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ð¼Ñƒ ÑÑÑ‹Ð»ÐºÐ¸

ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð½Ð°Ð´Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ ÑŽÐ·ÐµÑ€Ð°.

Ð’ Ñ‚Ð²Ð¾ÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚Ðµ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ»Ð¾Ð²Ð° Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ° (image, google, link)

Ð–ÑƒÑ€Ð½Ð°Ð» Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÐºÐ¸:

{text[-10000:]}
'''
    r = ai(query, temperature=0.1, model='gemini-1.5-flash-latest', tokens_limit=100)
    if 'image' in r.lower():
        result['image'] = True
    if 'google' in r.lower():
        result['google'] = True
    if 'link' in r.lower():
        result['link'] = True

    return result


if __name__ == '__main__':
    load_users_keys()

    # print(sum_big_text(open('1.txt', 'r', encoding='utf-8').read(), 'ÐŸÐµÑ€ÐµÑÐºÐ°Ð¶Ð¸ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¾ Ñ‡ÐµÐ¼ ÑÑ‚Ð¾Ñ‚ Ñ‚ÐµÐºÑÑ‚, ÑƒÐ»Ð¾Ð¶Ð¸ÑÑŒ Ð² 1000 ÑÐ»Ð¾Ð²'))

    # print(get_models())

    # print(get_mem_for_llama('test'))

    chat_cli()

    # for _ in range(100):
    #     t1 = time.time()
    #     r = ai('Ð½Ð°Ð¿Ð¸ÑˆÐ¸ Ñ€Ð°ÑÑÐºÐ°Ð· Ð¿Ñ€Ð¾ ÑÐ»Ð¾Ð½Ð° 4000 ÑÐ»Ð¾Ð²', temperature=1, model='gemini-1.5-flash-latest')
    #     t2 = time.time()
    #     print(len(r), round(t2 - t1, 2), f'{r[:20]}...{r[-20:]}'.replace('\n', ' '))

    # print(test_new_key('123'))

    # print(translate('Ù…Ø±Ø­Ø¨Ø§', to_lang='nl'))
    # print(translate('Î“ÎµÎ¹Î± ÏƒÎ±Ï‚', 'el', 'pt'))
    # print(translate('Ð“Ð¾Ð»Ð¾Ñ: Ynd Ð¼ÑƒÐ¶.', to_lang='en', help = 'ÑÑ‚Ð¾ Ñ‚ÐµÐºÑÑ‚ Ð½Ð° ÐºÐ½Ð¾Ð¿ÐºÐµ, Ð¾Ð½ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¼ Ñ‡Ñ‚Ð¾ Ð±Ñ‹ ÑƒÐ¼ÐµÑÑ‚Ð¸Ñ‚ÑÑ Ð½Ð° ÐºÐ½Ð¾Ð¿ÐºÐµ Ð¿Ð¾-ÑÑ‚Ð¾Ð¼Ñƒ ÑÐ¾ÐºÑ€Ð°Ñ‰ÐµÐ½, Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ - Ð“Ð¾Ð»Ð¾Ñ: Yandex Ð¼ÑƒÐ¶ÑÐºÐ¾Ð¹, Ñ‚ÑƒÑ‚ Ð¸Ð¼ÐµÐµÑ‚ÑÑ Ð² Ð²Ð¸Ð´Ñƒ Ð¼ÑƒÐ¶ÑÐºÐ¾Ð¹ Ð³Ð¾Ð»Ð¾Ñ Ð´Ð»Ñ TTS Ð¾Ñ‚ ÑÐ½Ð´ÐµÐºÑÐ°'))
    # print(translate('', to_lang='en'))
    # print(translate('', to_lang='en', help=''))

    # data = open('1.jpg', 'rb').read()
    # print(img2txt(data))

#     t = '''
# ð”ð’ð„ð‘: Ð¿Ñ€Ð¸Ð²ÐµÑ‚ ÐºÐ°Ðº Ð´ÐµÐ»Ð°
# ððŽð“: ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚, Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ñƒ Ð¼ÐµÐ½Ñ Ð½ÐµÑ‚ ÑÐ¼Ð¾Ñ†Ð¸Ð¹ Ð¸ Ñ‡ÑƒÐ²ÑÑ‚Ð², Ð½Ð¾ Ñ Ð³Ð¾Ñ‚Ð¾Ð² Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ñ‚ÐµÐ±Ðµ Ñ Ð»ÑŽÐ±Ñ‹Ð¼Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸ Ð¸Ð»Ð¸ Ð·Ð°Ð´Ð°Ñ‡Ð°Ð¼Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñƒ Ñ‚ÐµÐ±Ñ ÐµÑÑ‚ÑŒ. ÐšÐ°Ðº Ñ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ñ‚ÐµÐ±Ðµ ÑÐµÐ³Ð¾Ð´Ð½Ñ?

# ð”ð’ð„ð‘: Ñ‡Ñ‚Ð¾ Ñ‚Ñ‹ ÑƒÐ¼ÐµÐµÑˆÑŒ
# ððŽð“: Ð¯ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ñ Ñ€Ð°Ð·Ð½Ñ‹Ð¼Ð¸ Ð·Ð°Ð´Ð°Ñ‡Ð°Ð¼Ð¸ Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸. Ð’Ð¾Ñ‚ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹ Ñ‚Ð¾Ð³Ð¾, Ñ‡Ñ‚Ð¾ Ñ ÑƒÐ¼ÐµÑŽ:

# 1. **ÐžÑ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹**: Ñ Ð¼Ð¾Ð³Ñƒ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð½Ð° Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹, Ð¾Ñ‚ Ð½Ð°ÑƒÐºÐ¸ Ð¸ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð´Ð¾ ÐºÑƒÐ»ÑŒÑ‚ÑƒÑ€Ñ‹ Ð¸ Ñ€Ð°Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ð¹.
# 2. **Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚**: Ñ Ð¼Ð¾Ð³Ñƒ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚ Ð½Ð° Ð·Ð°Ð´Ð°Ð½Ð½ÑƒÑŽ Ñ‚ÐµÐ¼Ñƒ Ð¸Ð»Ð¸ Ð¿Ð¾ Ð¾Ð±Ñ€Ð°Ð·Ñ†Ñƒ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ‚Ñ‹ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸ÑˆÑŒ.
# 3. **ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚**: Ñ Ð¼Ð¾Ð³Ñƒ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚ Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° Ð½Ð° Ð´Ñ€ÑƒÐ³Ð¾Ð¹, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ñ€ÑƒÑÑÐºÐ¸Ð¹, Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ð¹, Ñ„Ñ€Ð°Ð½Ñ†ÑƒÐ·ÑÐºÐ¸Ð¹, Ð½ÐµÐ¼ÐµÑ†ÐºÐ¸Ð¹ Ð¸ Ð¼Ð½Ð¾Ð³Ð¸Ðµ Ð´Ñ€ÑƒÐ³Ð¸Ðµ ÑÐ·Ñ‹ÐºÐ¸.
# 4. **ÐŸÐ¾Ð¼Ð¾Ð³Ð°Ñ‚ÑŒ Ñ ÑÐ·Ñ‹ÐºÐ°Ð¼Ð¸**: Ñ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ñ Ð³Ñ€Ð°Ð¼Ð¼Ð°Ñ‚Ð¸ÐºÐ¾Ð¹, Ð»ÐµÐºÑÐ¸ÐºÐ¾Ð¹ Ð¸ ÑÐ¸Ð½Ñ‚Ð°ÐºÑÐ¸ÑÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐ¾Ð², Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ñ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸ÐµÐ¼ Ñ‚ÐµÐºÑÑ‚Ð¾Ð² Ð¸ Ð¿Ð¸ÑÑŒÐ¼Ð°Ð¼Ð¸.
# 5. **Ð ÐµÑˆÐ°Ñ‚ÑŒ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸**: Ñ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ñ Ñ€ÐµÑˆÐµÐ½Ð¸ÐµÐ¼ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð·Ð°Ð´Ð°Ñ‡, Ð¾Ñ‚ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ñ… Ð°Ñ€Ð¸Ñ„Ð¼ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ Ð´Ð¾ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡ Ð¿Ð¾ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¼Ñƒ Ð°Ð½Ð°Ð»Ð¸Ð·Ñƒ.
# 6. **Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð´ÐµÐ¸**: Ñ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹ Ð¸Ð´ÐµÐ¹ Ð´Ð»Ñ Ñ‚Ð²Ð¾Ñ€Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¾Ð², Ñ‚Ð°ÐºÐ¸Ñ… ÐºÐ°Ðº Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ñ€Ð°ÑÑÐºÐ°Ð·Ð¾Ð², ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÐµÐ² Ð¸Ð»Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð±Ð¸Ð·Ð½ÐµÑ-Ð¸Ð´ÐµÐ¹.
# 7. **ÐŸÐ¾Ð¼Ð¾Ð³Ð°Ñ‚ÑŒ Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÐµÐ¼**: Ñ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð², Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ð¾Ð² Ð¸Ð»Ð¸ Ð½Ð°Ð²Ñ‹ÐºÐ¾Ð², Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ñ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ¾Ð¹ Ðº ÑÐºÐ·Ð°Ð¼ÐµÐ½Ð°Ð¼.
# 8. **ÐžÐ±ÑÑƒÐ¶Ð´Ð°Ñ‚ÑŒ Ñ‚ÐµÐ¼Ñ‹**: Ñ Ð¼Ð¾Ð³Ñƒ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ñ‚ÑŒ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹, Ð¾Ñ‚ Ð½Ð°ÑƒÐºÐ¸ Ð¸ Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¹ Ð´Ð¾ Ð¸ÑÐºÑƒÑÑÑ‚Ð²Ð° Ð¸ ÐºÑƒÐ»ÑŒÑ‚ÑƒÑ€Ñ‹.

# Ð­Ñ‚Ð¾ Ð½Ðµ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ‚Ð¾Ð³Ð¾, Ñ‡Ñ‚Ð¾ Ñ ÑƒÐ¼ÐµÑŽ, Ð½Ð¾ Ñ Ð½Ð°Ð´ÐµÑŽÑÑŒ, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ Ð´Ð°ÑÑ‚ Ñ‚ÐµÐ±Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾ Ð¼Ð¾Ð¸Ñ… Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑÑ…. Ð•ÑÐ»Ð¸ Ñƒ Ñ‚ÐµÐ±Ñ ÐµÑÑ‚ÑŒ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¸Ð»Ð¸ Ð·Ð°Ð´Ð°Ñ‡Ð°, Ñ Ð³Ð¾Ñ‚Ð¾Ð² Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ!

# ð”ð’ð„ð‘: Ð° Ð¼Ð¾Ð¶ÐµÑˆÑŒ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ Ð±Ð°Ð±Ð¾Ñ‡ÐºÐ¸?
# ððŽð“: Ð¯ Ð½Ðµ Ð¼Ð¾Ð³Ñƒ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ, Ñ‚Ð°ÐºÐ¸Ðµ ÐºÐ°Ðº ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸, Ð½Ð¾ Ñ Ð¼Ð¾Ð³Ñƒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ASCII-Ð°Ñ€Ñ‚, Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾ÑÑ‚ÑƒÑŽ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ Ð±Ð°Ð±Ð¾Ñ‡ÐºÐ¸ Ð´Ð»Ñ Ñ‚ÐµÐ±Ñ!

# Ð’Ð¾Ñ‚ Ð¼Ð¾Ñ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð±Ð°Ð±Ð¾Ñ‡ÐºÑƒ Ð¸Ð· ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²:
# ```
#  /__/\
# ( o.o )
#  > ^ <
# ```
# ÐÐ°Ð´ÐµÑŽÑÑŒ, Ñ‚ÐµÐ±Ðµ Ð¿Ð¾Ð½Ñ€Ð°Ð²Ð¸Ð»Ð°ÑÑŒ Ð¼Ð¾Ñ Ð±Ð°Ð±Ð¾Ñ‡ÐºÐ°!

# ð”ð’ð„ð‘: ÐºÐ°Ðº Ð·Ð¾Ð²ÑƒÑ‚ ÐºÐ¾ÑÐ¼Ð¸Ñ‡ÐµÑÐºÑƒÑŽ ÐºÐ¾Ñ€Ð¾Ð²Ñƒ Ð¸Ð· ÐºÐ½Ð¸Ð³Ð¸ Ð¿Ñ€Ð¸ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ð°Ð»Ð¸ÑÑ‹?
# '''
#     print(detect_intent(t))
