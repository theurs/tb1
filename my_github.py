#!/usr/bin/env python3
# https://github.com/settings/tokens
# https://github.com/marketplace/models/azure-openai/gpt-4o-mini/playground
# https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits


import base64
import os
import time
import threading
import traceback

from openai import OpenAI
from sqlitedict import SqliteDict

import cfg
import my_db
import my_log
import utils


BASE_URL = 'https://models.inference.ai.azure.com'

# —Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ —Ö—Ä–∞–Ω–∏—Ç—å
MAX_MEM_LINES = 20
MAX_HIST_CHARS = 12000
MAX_HIST_CHARS_4K = 6000

# –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —á–∞—Ç–æ–≤ —á—Ç–æ –±—ã –Ω–µ –∏—Å–ø–æ—Ä—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é
# {id:lock}
LOCKS = {}

# –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã –±–æ–ª—å—à–µ —á–µ–º, —ç—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç–∞, –≤ —ç—Ç–æ–º –º–æ–¥—É–ª–µ –æ–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
MAX_REQUEST = 8000
KEYS_LOCK = threading.Lock()


BIG_GPT_MODEL = 'gpt-4o'
DEFAULT_MODEL = 'gpt-4o-mini'

BIG_GPT_41_MODEL = 'gpt-4.1'
DEFAULT_41_MINI_MODEL = 'gpt-4.1-mini'

# —É –≥—Ä–æ–∫–∞ –ª–∏–º–∏—Ç –≤—Å–µ–≥–æ 4–∫ —Ç–æ–∫–µ–Ω–æ–≤
GROK_MODEL = 'grok-3'
GROK_MODEL_FALLBACK = 'grok-3-mini'

DEEPSEEK_R1_MODEL = 'DeepSeek-R1'
DEEPSEEK_R1_MODEL_FALLBACK = 'gpt-4o-mini'


TEXT_TO_SPEECH_MS_PHI_4_MODEL = 'microsoft/Phi-4-multimodal-instruct'


CURRENT_KEY_SET = []

# –∫–∞–∂–¥—ã–π —é–∑–µ—Ä –¥–∞–µ—Ç —Å–≤–æ–∏ –∫–ª—é—á–∏ –∏ –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å–æ –≤—Å–µ–º–∏
# –∫–∞–∂–¥—ã–π –∫–ª—é—á –¥–∞–µ—Ç –≤—Å–µ–≥–æ 50/150 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å—É—Ç–∫–∏ —Ç–∞–∫ —á—Ç–æ —á–µ–º –±–æ–ª—å—à–µ —Ç–µ–º –ª—É—á—à–µ
# {full_chat_id as str: key}
# {'[9123456789] [0]': 'key', ...}
ALL_KEYS = []
USER_KEYS = SqliteDict('db/github_user_keys.db', autocommit=True)
USER_KEYS_LOCK = threading.Lock()


def get_next_key() -> str:
    """
    Retrieves the next API key in a round-robin fashion.

    Returns:
        str: The next available API key.

    Raises:
        Exception: If no API keys are available.
    """
    global CURRENT_KEY_SET
    with KEYS_LOCK:
        if not CURRENT_KEY_SET:
            if ALL_KEYS:
                CURRENT_KEY_SET = ALL_KEYS[:]
        if CURRENT_KEY_SET:
            return CURRENT_KEY_SET.pop(0)
        else:
            raise Exception('No api keys')


def clear_mem(mem: list, user_id: str) -> list:
    """
    Clears the memory (chat history) for a given user, ensuring it does not exceed the maximum allowed size.

    Args:
        mem (list): The chat history list.
        user_id (str): The user's ID.

    Returns:
        list: The trimmed chat history.
    """
    while True:
        sizeofmem = count_tokens(mem)
        if sizeofmem <= MAX_HIST_CHARS:
            break
        try:
            mem = mem[2:]
        except IndexError:
            mem = []
            break

    return mem[-MAX_MEM_LINES * 2 :]


def clear_mem2(mem: list, user_id: str) -> list:
    """
    Clears the memory (chat history) for a given user, ensuring it does not exceed the maximum allowed size.
    Version for 4k context (DeepSeek R1)

    Args:
        mem (list): The chat history list.
        user_id (str): The user's ID.

    Returns:
        list: The trimmed chat history.
    """
    while True:
        sizeofmem = count_tokens(mem)
        if sizeofmem <= MAX_HIST_CHARS_4K:
            break
        try:
            mem = mem[2:]
        except IndexError:
            mem = []
            break

    return mem[-MAX_MEM_LINES * 2 :]


def count_tokens(mem: list) -> int:
    """
    Counts the total number of tokens in the chat history.

    Args:
        mem (list): The chat history list.

    Returns:
        int: The total number of tokens.
    """
    return sum([len(m['content']) for m in mem])


def ai(
    prompt: str = '',
    mem = None,
    user_id: str = '',
    system: str = '',
    model = DEFAULT_MODEL, # gpt-4o-mini, gpt-4o, DeepSeek-R1,...
    temperature: float = 1,
    max_tokens: int = 4000,
    timeout: int = 120,
    key_: str = None
    ) -> str:

    temperature = temperature/2

    if not model:
        model = DEFAULT_MODEL

    if not prompt and not mem:
        return ''

    mem_ = mem[:] if mem else []

    # current date time string
    now = utils.get_full_time()
    systems = (
        f'Current date and time: {now}\n',
        'Ask again if something is unclear in the request',
        'You (assistant) are currently working in a Telegram bot. The Telegram bot automatically extracts text from any type of files sent to you by the user, such as documents, images, audio recordings, etc., so that you can fully work with any files.',
        # 'You (assistant) can use some telegram bot functions, answer starting /tts lang text-to-say (example: /tts ru –ü—Ä–∏–≤–µ—Ç) and user will get voice message from you, answer starting /img detailed-professional-description-of-an-image and user will get images generated by ai.',
        "If the user's request cannot be fulfilled using the available tools or direct actions, the assistant(you) must treat the request as a request to generate text (e.g., providing code as text), not a request to perform an action (e.g., executing code or interacting with external systems not directly supported by tools) (intention mismatch).",
        "To edit image user can send image with caption starting ! symbol",
    )

    if system:
        mem_.insert(0, {"role": "system", "content": system})
    for s in reversed(systems):
        mem_.insert(0, {"role": "system", "content": s})
    if prompt:
        mem_ = mem_ + [{'role': 'user', 'content': prompt}]

    text = ''

    start_time = time.time()

    key = ''

    for _ in range(3):
        if time.time() - start_time > timeout:
            return ''

        try:
            key = key_ or get_next_key()
            client = OpenAI(
                api_key = key,
                base_url = BASE_URL,
                )
            response = client.chat.completions.create(
                messages = mem_,
                model = model,
                max_tokens = max_tokens,
                temperature = temperature,
                timeout = timeout,
                )
        except Exception as error_other:
            if 'Bad credentials' in str(error_other) or 'Your account type is not currently supported' in str(error_other):
                remove_key(key)
                continue
            if 'tokens_limit_reached' in str(error_other):
                # —Å–Ω—è—Ç—å –ø–µ—Ä–≤—ã–µ 2 –∑–∞–ø–∏—Å–∏ –∏–∑ mem
                try:
                    mem__ = mem[2:]
                except IndexError:
                    return ''
                return ai(prompt, mem__, user_id, system, model, temperature, max_tokens, timeout, key_)

            if "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry." not in str(error_other):
                my_log.log_github(f'ai:1: {error_other}\n{key}')
            return ''

        try:
            text = response.choices[0].message.content
            break
        except Exception as error:
            traceback_error = traceback.format_exc()
            my_log.log_github(f'Failed to parse response: {error}\n\n{str(response)}\n{key}\n\n{traceback_error}')
            text = ''
            if key_:
                break
            time.sleep(2)

    if text:
        return text.strip()

    return ''


def remove_key(key: str):
    '''
    Removes a given key from the ALL_KEYS list and from the USER_KEYS dictionary.
    '''
    try:
        if key in ALL_KEYS:
            try:
                ALL_KEYS.remove(key) # Use remove for safer deletion by value
            except ValueError:
                my_log.log_keys(f'remove_key: Invalid key {key} not found in ALL_KEYS list') # Log if key not found

        keys_to_delete = [] # List to store user keys for deletion
        with USER_KEYS_LOCK:
            # remove key from USER_KEYS
            for user in USER_KEYS:
                if USER_KEYS[user] == key:
                    keys_to_delete.append(user) # Add user key to deletion list

            for user_key in keys_to_delete: # Iterate over deletion list after initial iteration
                del USER_KEYS[user_key] # Safely delete keys

            if keys_to_delete:
                my_log.log_keys(f'github: Invalid key {key} removed from users {keys_to_delete}') # Log removed keys with users
            else:
                my_log.log_keys(f'github: Invalid key {key} was not associated with any user in USER_KEYS') # Log if key not found in USER_KEYS

    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_github(f'Failed to remove key {key}: {error}\n\n{error_traceback}')


def update_mem(query: str, resp: str, chat_id: str):
    """
    Updates the memory with the user query and assistant response.

    Args:
        query (str): The user query.
        resp (str): The assistant response.
        chat_id (str): The chat ID.

    Returns:
        None
    """
    mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []
    mem += [{'role': 'user', 'content': query}]
    mem += [{'role': 'assistant', 'content': resp}]
    mem = clear_mem(mem, chat_id)

    mem__ = []
    try:
        i = 0
        while i < len(mem):
            if i == 0 or mem[i] != mem[i-1]:
                mem__.append(mem[i])
            i += 1
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_github(f'my_github:update_mem: {error}\n\n{error_traceback}\n\n{query}\n\n{resp}\n\n{mem}')

    my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem__))


def chat(query: str, chat_id: str = '', temperature: float = 1, system: str = '', model: str = '', max_tokens: int = 4000) -> str:
    """
    Handles chat interaction with the AI model.

    Args:
        query (str): The user query.
        chat_id (str, optional): The chat ID. Defaults to ''.
        temperature (float, optional): The temperature for AI model. Defaults to 1.
        system (str, optional): The system message for AI model. Defaults to ''.
        model (str, optional): The AI model to use. Defaults to ''.

    Returns:
        str: The response from the AI model, or an empty string in case of failure.
    """
    if chat_id in LOCKS:
        lock = LOCKS[chat_id]
    else:
        lock = threading.Lock()
        LOCKS[chat_id] = lock
    with lock:
        mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []

        text = ai(query, mem, user_id=chat_id, temperature = temperature, system=system, model=model, max_tokens=max_tokens)

        if text:
            my_db.add_msg(chat_id, model or DEFAULT_MODEL)
            mem += [{'role': 'user', 'content': query}]
            mem += [{'role': 'assistant', 'content': text}]
            if model == DEEPSEEK_R1_MODEL:
                mem = clear_mem2(mem, chat_id)
            else:
                mem = clear_mem(mem, chat_id)
            my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem))
        return text
    return ''


def chat_cli(model: str = ''):
    """
    Command-line interface for interacting with the chat function.

    Args:
        model (str, optional): The AI model to use. Defaults to ''.

    Returns:
        None
    """
    reset('test')
    while 1:
        q = input('>')
        if q == 'mem':
            print(get_mem_as_string('test'))
            continue
        r = chat(q, 'test', model = model, system='–æ—Ç–≤–µ—á–∞–π –≤—Å–µ–≥–¥–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º')
        print(r)


def force(chat_id: str, text: str) -> None:
    """
    Updates the last bot answer in the chat history with the given text.

    Args:
        chat_id (str): The ID of the chat to update.
        text (str): The new text to replace the last bot answer with.

    Returns:
        None
    """
    try:
        if chat_id in LOCKS:
            lock = LOCKS[chat_id]
        else:
            lock = threading.Lock()
            LOCKS[chat_id] = lock
        with lock:
            mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or [] # Retrieve chat memory from database
            if mem and len(mem) > 1: # Check if memory exists and has at least two messages (user and bot message)
                mem[-1]['content'] = text # Update the content of the last message (assuming it's the bot's last response)
                my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem)) # Update the chat memory in the database
    except Exception as error: # Catch any exceptions during the process
        error_traceback = traceback.format_exc() # Get full traceback of the error
        my_log.log_github(f'Failed to force message in chat {chat_id}: {error}\n\n{error_traceback}') # Log error details to GitHub


def undo(chat_id: str) -> None:
    """
    Removes the last two messages (user and bot) from the chat history for a given chat ID.

    Args:
        chat_id (str): The ID of the chat to undo the last messages from.

    Returns:
        None
    """
    try:
        if chat_id in LOCKS:
            lock = LOCKS[chat_id]
        else:
            lock = threading.Lock()
            LOCKS[chat_id] = lock
        with lock:
            mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or [] # Retrieve chat memory from database
            mem = mem[:-2] # Remove the last two messages from the memory list
            my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem)) # Update the chat memory in the database
    except Exception as error: # Catch any exceptions during the process
        error_traceback = traceback.format_exc() # Get full traceback of the error
        my_log.log_github(f'Failed to undo chat {chat_id}: {error}\n\n{error_traceback}') # Log error details to GitHub


def reset(chat_id: str) -> None:
    """
    Resets the chat history for the specified chat ID by clearing the memory.

    Args:
        chat_id (str): The ID of the chat to reset.

    Returns:
        None
    """
    mem = [] # Initialize an empty list to represent empty memory
    my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem)) # Update the chat memory in the database with empty memory, effectively resetting the chat


def get_mem_as_string(chat_id: str, md: bool = False) -> str:
    """
    Returns the chat history as a string for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to get the history for.

    Returns:
        str: The chat history as a string.
    """
    try:
        mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []
        result = ''
        for x in mem:
            role = x['role']
            if role == 'user': role = 'ùêîùêíùêÑùêë'
            if role == 'assistant': role = 'ùêÅùêéùêì'
            if role == 'system': role = 'ùêíùêòùêíùêìùêÑùêå'
            text = x['content']
            if text.startswith('[Info to help you answer'):
                end = text.find(']') + 1
                text = text[end:].strip()
            if md:
                result += f'{role}:\n\n{text}\n\n'
            else:
                result += f'{role}: {text}\n'
            if role == 'ùêÅùêéùêì':
                if md:
                    result += '\n\n'
                else:
                    result += '\n'
        return result 
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_github(f'get_mem_as_string: {error}\n\n{error_traceback}')
        return ''


def img2txt(
    image_data: bytes,
    prompt: str = 'Describe picture',
    model = DEFAULT_MODEL,
    temperature: float = 1,
    max_tokens: int = 2000,
    timeout: int = 120,
    chat_id: str = '',
    system: str = '',
    ) -> str:
    """
    Describes an image using the specified model and parameters.

    Args:
        image_data: The image data as bytes.
        prompt: The prompt to guide the description. Defaults to 'Describe picture'.
        model: The model to use for generating the description. Defaults to 'mistralai/pixtral-12b:free'.
        temperature: The temperature parameter for controlling the randomness of the output. Defaults to 1.
        max_tokens: The maximum number of tokens to generate. Defaults to 2000.
        timeout: The timeout for the request in seconds. Defaults to 120.

    Returns:
        A string containing the description of the image, or an empty string if an error occurs.
    """
    temperature = temperature/2
    if not model:
        model = DEFAULT_MODEL
    if not prompt:
        prompt = 'Describe picture'

    if isinstance(image_data, str):
        with open(image_data, 'rb') as f:
            image_data = f.read()

    img_b64_str = base64.b64encode(image_data).decode('utf-8')
    img_type = 'image/png'

    mem = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:{img_type};base64,{img_b64_str}"},
                },
            ],
        }
    ]
    if system:
        mem.insert(0, {'role': 'system', 'content': system})

    result = ''

    for _ in range(3):
        try:
            key = get_next_key()
            client = OpenAI(
                api_key = key,
                base_url = BASE_URL,
                )
            response = client.chat.completions.create(
                messages = mem,
                model = model,
                max_tokens = max_tokens,
                temperature = temperature,
                timeout = timeout,
                )
        except Exception as error_other:
            if 'Bad credentials' in str(error_other) or 'Your account type is not currently supported' in str(error_other):
                remove_key(key)
                continue
            my_log.log_github(f'ai:2: {error_other}')
            return ''

        try:
            result = response.choices[0].message.content
            break
        except Exception as error:
            my_log.log_github(f'Failed to parse response: {error}\n\n{str(response)}')
            result = ''
            time.sleep(2)

    if chat_id:
        my_db.add_msg(chat_id, model)

    return result


def load_users_keys():
    """
    Load users' keys into memory and update the list of all keys available.
    """
    with USER_KEYS_LOCK:
        global ALL_KEYS
        ALL_KEYS = cfg.GITHUB_TOKENS if hasattr(cfg, 'GITHUB_TOKENS') and cfg.GITHUB_TOKENS else []
        for user in USER_KEYS:
            key = USER_KEYS[user]
            if key not in ALL_KEYS:
                ALL_KEYS.append(key)


def test_key(key: str) -> bool:
    '''
    Tests a given key by making a simple request to the GitHub AI API.
    '''
    r = ai('1+1=', key_=key.strip())
    return bool(r)


import base64
import io
import time
import traceback

from openai import OpenAI

# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Å–ª–µ–¥—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –æ–±—ä–µ–∫—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –∏–∑ –≤–∞—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:
# get_next_key, remove_key, my_log, my_db, utils

def transcribe_audio_with_model(
    audio_data: bytes | str,
    user_id: str = '',
    model: str = 'Phi-4-multimodal-instruct',
    timeout: int = 120,
    max_tokens: int = 1000,
    temperature: float = 0.2,
    ) -> str:
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã–µ –≤ —Ç–µ–∫—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É—è —É–∫–∞–∑–∞–Ω–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        audio_data (bytes | str): –ê—É–¥–∏–æ–¥–∞–Ω–Ω—ã–µ –≤ –≤–∏–¥–µ –±–∞–π—Ç–æ–≤ –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞,
                                  –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è –ø—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, '.ogg', '.flac').
        user_id (str): ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è Telegram, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è API-–∫–ª—é—á–∞–º–∏.
        model (str): –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'Phi-4-multimodal-instruct'.
        timeout (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç API –≤ —Å–µ–∫—É–Ω–¥–∞—Ö. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 120.
        max_tokens (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –æ—Ç–≤–µ—Ç–µ. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 4000.
        temperature (float): –ü–∞—Ä–∞–º–µ—Ç—Ä –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.2.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        str: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    text = ''
    key = ''
    start_time = time.time()

    audio_bytes = None
    audio_b64_str = ''
    audio_type = 'audio/wav'

    try:
        if isinstance(audio_data, str):
            with open(audio_data, 'rb') as f:
                audio_bytes = f.read()
        elif isinstance(audio_data, bytes):
            audio_bytes = audio_data
        else:
            # –ó–∞–º–µ–Ω—è–µ–º raise –Ω–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—Ç –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏
            my_log.log_github(f'transcribe_audio_with_model: Invalid audio_data type for user {user_id}. Must be bytes or a file path string.')
            return ''

        audio_b64_str = base64.b64encode(audio_bytes).decode('utf-8')

    except FileNotFoundError:
        my_log.log_github(f'transcribe_audio_with_model: Audio file not found at path {audio_data} for user {user_id}.')
        return ''
    except Exception as e:
        traceback_error = traceback.format_exc()
        my_log.log_github(f'transcribe_audio_with_model: Audio processing error for user {user_id}: {e}\n{traceback_error}')
        return ''

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "audio_url", "audio_url": {"url": f"data:{audio_type};base64,{audio_b64_str}"}},
                {"type": "text", "text": "Transcribe the audio clip into text."},
            ],
        }
    ]

    for _ in range(3):
        if time.time() - start_time > timeout:
            my_log.log_github(f"transcribe_audio_with_model: Timeout after {timeout} seconds for user {user_id}.")
            return ''

        try:
            key = get_next_key()
            client = OpenAI(
                api_key = key,
                base_url = BASE_URL,
            )

            response = client.chat.completions.create(
                messages = messages,
                model = model,
                max_tokens = max_tokens,
                temperature = temperature,
                timeout = timeout,
            )
            text = response.choices[0].message.content
            if text:
                text = text.strip()
                break
            else:
                time.sleep(2)

        except Exception as error_other:
            traceback_error = traceback.format_exc()
            my_log.log_github(f'transcribe_audio_with_model: API Error for user {user_id}: {error_other}\n{traceback_error}\nKey: {key}')

            if 'Bad credentials' in str(error_other) or 'Your account type is not currently supported' in str(error_other):
                remove_key(key)
                continue

            if "The response was filtered due to the prompt triggering Azure OpenAI's content management policy." not in str(error_other):
                my_log.log_github(f'transcribe_audio_with_model: Non-credential error for user {user_id}: {error_other}')
            return ''

    if text and user_id:
        my_db.add_msg(user_id, model)

    return text or ''


def test_transcribe():
    # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º–∏
    audio_dir = r'C:\Users\user\Downloads'
    test_user_id = 'test_user_audio_transcription'

    if not os.path.isdir(audio_dir):
        print(f"–û—à–∏–±–∫–∞: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {audio_dir}")
    else:
        print(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –∏–∑: {audio_dir}")
        for filename in os.listdir(audio_dir):
            file_path = os.path.join(audio_dir, filename)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Ñ–∞–π–ª–æ–º –∏ –∏–º–µ–µ—Ç –ª–∏ –æ–Ω –∞—É–¥–∏–æ—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            if os.path.isfile(file_path) and filename.lower().endswith(('.ogg', '.mp3', '.wav', '.flac')):
                print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {filename}")
                transcribed_text = transcribe_audio_with_model(
                    audio_data=file_path,
                    user_id=test_user_id,
                    # model='Phi-4-multimodal-instruct' # –£–∫–∞–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å —è–≤–Ω–æ
                )

                if transcribed_text:
                    output_filename = os.path.splitext(filename)[0] + '.txt'
                    output_file_path = os.path.join(audio_dir, output_filename)
                    with open(output_file_path, 'w', encoding='utf-8') as f:
                        f.write(transcribed_text)
                    print(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_file_path}")
                else:
                    print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª: {filename}")
            else:
                print(f"–ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª (–Ω–µ –∞—É–¥–∏–æ –∏–ª–∏ –Ω–µ —Ñ–∞–π–ª): {filename}")

        print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è chat_cli, –µ—Å–ª–∏ –æ–Ω –Ω—É–∂–µ–Ω –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ç–µ—Å—Ç–æ–≤
    # chat_cli(model = GROK_MODEL)


if __name__ == '__main__':
    pass
    my_db.init(backup=False)
    load_users_keys()

    test_transcribe()

    # t = '–Ω–∞–ø–∏—à–∏ —Å—Ç–∏—Ö –æ —Ä–æ–¥–∏–Ω–µ —Å–ª–æ–Ω–æ–≤ —Å —Ö–æ—Ä–æ—à–µ–π —Ä–∏—Ñ–º–æ–π'
    # print(ai(t, model=GROK_MODEL))

    # test_key('')

    # reset('test')
    chat_cli(model = GROK_MODEL)

    # print(img2txt(r'C:\Users\user\Downloads\samples for ai\–∫–∞—Ä—Ç–∏–Ω–∫–∏\–º–∞—Ç –∑–∞–¥–∞—á–∏.jpg', '—Ä–µ—à–∏ –∑–∞–¥–∞—á–∏, –≤ –æ—Ç–≤–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–π —é–Ω–∏–∫–æ–¥ —Å–∏–º–≤–æ–ª—ã –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –≤–º–µ—Å—Ç–æ latex –≤—ã—Ä–∞–∂–µ–Ω–∏–π', model = GROK_MODEL, temperature=0))
    # print(voice2txt('C:/Users/user/Downloads/1.ogg'))

    my_db.close()
