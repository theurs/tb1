#!/usr/bin/env python3
# install from PyPI
# pip install groq

import base64
import cachetools.func
import random
import re
import requests
import time
import threading
import traceback
from typing import Union

import httpx
from groq import Groq, PermissionDeniedError
from sqlitedict import SqliteDict

import cfg
import my_db
import my_log
import my_skills
import my_skills_storage
import my_sum
import utils


# –∫–∞–∂–¥—ã–π —é–∑–µ—Ä –¥–∞–µ—Ç —Å–≤–æ–∏ –∫–ª—é—á–∏ –∏ –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å–æ –≤—Å–µ–º–∏
# –∫–∞–∂–¥—ã–π –∫–ª—é—á –¥–∞–µ—Ç –≤—Å–µ–≥–æ 6000 —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º–∏–Ω—É—Ç—É –¥–ª—è –ª–ª–∞–º–∞3 —Ç–∞–∫ —á—Ç–æ —á–µ–º –±–æ–ª—å—à–µ —Ç–µ–º –ª—É—á—à–µ
# {full_chat_id as str: key}
# {'[9123456789] [0]': 'key', ...}
USER_KEYS = SqliteDict('db/groq_user_keys.db', autocommit=True)
# list of all users keys
ALL_KEYS = []
USER_KEYS_LOCK = threading.Lock()


# for ai func
# DEFAULT_MODEL = 'llama-3.2-90b-vision-preview'
DEFAULT_MODEL = 'meta-llama/llama-4-maverick-17b-128e-instruct'
FALLBACK_MODEL = 'meta-llama/llama-4-scout-17b-16e-instruct'


# –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —á–∞—Ç–æ–≤ —á—Ç–æ –±—ã –Ω–µ –∏—Å–ø–æ—Ä—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é 
# {id:lock}
LOCKS = {}

# –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã –±–æ–ª—å—à–µ —á–µ–º, —ç—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç–∞, –≤ —ç—Ç–æ–º –º–æ–¥—É–ª–µ –æ–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
MAX_REQUEST = 6000
MAX_REQUEST_LLAMA31 = 20000

MAX_QUERY_LENGTH = 10000
MAX_MEM_LLAMA31 = 50000
# –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–º—è—Ç–∏
MAX_LINES = 20

# limit for summarize
MAX_SUM_REQUEST = MAX_MEM_LLAMA31


# –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–∏–ø—Å–∏–∫ –ª–ª–∞–º—ã
MAX_QWQ32B_chars = 10000
MAX_REQUEST_deepseek_r1_distill_llama70b = 4000
MAX_REQUEST_qwq32b = 4000
DEEPSEEK_LLAMA70_MODEL = 'deepseek-r1-distill-llama-70b'
DEEPSEEK_QWQ32B_MODEL = 'qwen-qwq-32b'


CURRENT_KEY_SET = []


def get_next_key() -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π –∫–ª—é—á –∏–∑ —Å–ø–∏—Å–∫–∞ ALL_KEYS."""
    global CURRENT_KEY_SET

    with USER_KEYS_LOCK:
        if not CURRENT_KEY_SET:
            CURRENT_KEY_SET = ALL_KEYS[:]
        if CURRENT_KEY_SET:
            return CURRENT_KEY_SET.pop(0)
        else:
            raise Exception('No more keys available')


def encode_image(image_data: bytes) -> str:
    """–ö–æ–¥–∏—Ä—É–µ—Ç –±–∞–π—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Å—Ç—Ä–æ–∫—É Base64."""
    try:
        result = base64.b64encode(image_data).decode('utf-8')
        return result
    except Exception as error:
        my_log.log_groq(f'encode_image: error: {error}')
        return ''


def img2txt(image_data: Union[str, bytes],
            prompt: str = "What's in this image?", 
            timeout: int = 60,
            model = 'llava-v1.5-7b-4096-preview',
            key_: str = '',
            json_output=False,
            temperature: float = 1,
            chat_id: str = '',
            system: str = '',
            ) -> str:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –º–æ–¥–µ–ª—å LLaVA –∏ –ø–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ.

    Args:
        image_data: –ò–º—è —Ñ–∞–π–ª–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Å—Ç—Ä–æ–∫–∞) –∏–ª–∏ –±–∞–π—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        prompt: –ü–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏.
        timeout: –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö).
        model: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –º–æ–¥–µ–ª–∏ LLaVA.
        _key: –ö–ª—é—á API Groq (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π). –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, 
              –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Å–ª—É—á–∞–π–Ω—ã–π –∫–ª—é—á –∏–∑ —Å–ø–∏—Å–∫–∞ ALL_KEYS.
        temperature: —Ç–µ–º–ø—Ä–µ—Ä–∞—Ç—É—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ —ç—Ç–æ llama 3+ —Ç–æ –±—É–¥–µ—Ç –ø–æ–¥–µ–ª–µ–Ω–∞ –Ω–∞ 2
        chat_id: –¥–ª—è —É—á–µ—Ç–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–æ–±—â–µ–Ω–∏–π

    Returns:
        –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –æ—Ç –º–æ–¥–µ–ª–∏. 
        –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –∏–ª–∏ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞.
    """

    if 'llama-3' in model.lower():
        temperature = temperature / 2

    if isinstance(image_data, str):
        with open(image_data, 'rb') as f:
            image_data = f.read()

    if json_output:
        resp_type = 'json_object'
    else:
        resp_type = 'text'

    # Getting the base64 string
    base64_image = encode_image(image_data)
    mem = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}",
                    },
                },
            ],
        }
    ]
    if system:
        mem.insert(0, {'role': 'system', 'content': system})

    x = 0
    while x < 4:
        x += 1
        if key_:
            key = key_
            x = 4
        else:
            key = get_next_key()
        try:
            client = Groq(api_key=key, timeout = timeout)

            chat_completion = client.chat.completions.create(
                messages=mem,
                model=model,
                response_format = {"type": resp_type},
                temperature = temperature,
            )

            result = chat_completion.choices[0].message.content.strip()
            if result:
                my_db.add_msg(chat_id, model)
                return result
        except Exception as error:
            error_traceback = traceback.format_exc()
            my_log.log_groq(f'my_groq:img2txt: {error}\n{error_traceback}')

        time.sleep(2)

    return ''


def ai(prompt: str = '',
       system: str = '',
       mem_ = [],
       temperature: float = 1,
       model_: str = '',
       max_tokens_: int = 4000,
       key_: str = '',
       timeout: int = 180,
       json_output: bool = False,
       user_id: str = '',
       ) -> str:
    """
    Generates a response using the GROQ AI model.

    Args:
        prompt (str, optional): The user's input prompt. Defaults to ''.
        system (str, optional): The system's initial message. Defaults to ''.
        mem_ (list, optional): The list of previous messages. Defaults to [].
        temperature (float, optional): The randomness of the generated response. Defaults to 1.
        model_ (str, optional): The name of the GROQ model to use. Defaults to 'llama3-70b-8192'.
            (llama3-8b-8192, mixtral-8x7b-32768, llama-3.1-405b-reasoning, llama-3.1-70b-versatile, llama-3.1-8b-instant)
        max_tokens_ (int, optional): The maximum number of tokens in the generated response. Defaults to 2000.
        key_ (str, optional): The API key for the GROQ model. Defaults to ''.
        timeout (int, optional): The timeout for the request in seconds. Defaults to 120.
        json_output (bool, optional): Whether to return the response as a JSON object. Defaults to False.
        user_id (str, optional): The user's ID. Defaults to ''.

    Returns:
        str: The generated response from the GROQ AI model. Returns an empty string if error.

    Raises:
        Exception: If an error occurs during the generation of the response. The error message and traceback are logged.
    """
    try:
        mem = []
        if mem_:
            if system:
                mem.append({'role': 'system', 'content': system})
                mem += mem_
                if prompt:
                    mem.append({'role': 'user', 'content': prompt})
            else:
                mem = mem_
                if prompt:
                    mem.append({'role': 'user', 'content': prompt})
        else:
            if system:
                mem.append({'role': 'system', 'content': system})
            if prompt:
                mem.append({'role': 'user', 'content': prompt})

        if not mem:
            return ''

        # model="llama3-70b-8192", # llama3-8b-8192, mixtral-8x7b-32768, 'llama-3.1-70b-versatile' 'llama-3.1-405b-reasoning'
        model = model_ if model_ else DEFAULT_MODEL

        max_mem = MAX_QUERY_LENGTH
        if 'llama-3.1' in model:
            max_mem = MAX_MEM_LLAMA31
        elif model_ in  ('deepseek-r1-distill-llama-70b', 'qwen-qwq-32b'):
            max_mem = MAX_QWQ32B_chars
        while token_count(mem) > max_mem + 100:
            mem = mem[2:]

        if 'llama' in model_.lower() or 'llama' in model_.lower() or 'qwen' in model_.lower():
            temperature = temperature / 2

        x = 0
        start_time = time.time()
        timeout_init = timeout
        while x < 4:

            if time.time() - start_time > timeout_init:
                return ''

            x += 1
            if key_:
                key = key_
                x = 4
            else:
                key = get_next_key()

            timeout = timeout_init - (time.time() - start_time)
            if timeout < 5:
                return ''

            if hasattr(cfg, 'GROQ_PROXIES') and cfg.GROQ_PROXIES:
                client = Groq(
                    api_key=key,
                    http_client = httpx.Client(proxy = random.choice(cfg.GROQ_PROXIES)),
                    timeout = 5,
                )
            else:
                client = Groq(api_key=key, timeout = timeout)

            try:
                params = {
                    "messages": mem,
                    "model": model,
                    "temperature": temperature,
                    "max_tokens": max_tokens_,
                    "timeout": timeout,
                }
                
                # Try with JSON mode first if requested
                if json_output:
                    json_params = params.copy()
                    json_params["response_format"] = {"type": "json_object"}
                    try:
                        chat_completion = client.chat.completions.create(**json_params)
                    except Exception as json_error:
                        # Fallback to text mode if JSON validation fails
                        if 'json_validate_failed' in str(json_error):
                            my_log.log_groq(f'GROQ JSON mode failed for model {model}. Retrying in text mode.')
                            params["response_format"] = {"type": "text"}
                            chat_completion = client.chat.completions.create(**params)
                        else:
                            raise json_error  # Re-raise other errors
                else:
                    # Standard text request
                    chat_completion = client.chat.completions.create(**params)

            except PermissionDeniedError:
                my_log.log_groq(f'GROQ PermissionDeniedError: {key}')
                continue
            except Exception as error:
                if 'invalid api key' in str(error).lower() or 'Organization has been restricted' in str(error):
                    remove_key(key)
                    continue
                if 'Rate limit reached for model' in str(error).lower():
                    continue
                if "'message': 'Request Entity Too Large', 'type': 'invalid_request_error', 'code': 'request_too_large'" in str(error):
                    return str(error)
                my_log.log_groq(f'GROQ {error} {key} {model} {str(mem)[:1000]}')
            try:
                resp = chat_completion.choices[0].message.content.strip()

                # —ç—Ç–∞ –º–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç —á–µ—Ä–µ–∑ reasoning –≤–º–µ—Å—Ç–æ context?
                if 'compound' in chat_completion.model:
                    if chat_completion.choices[0].message.reasoning:
                        resp += '\n\n' + chat_completion.choices[0].message.reasoning

                # –ø—Ä–æ–≤–µ—Ä–∫–∞ –µ—Å—Ç—å –ª–∏ –≤ –æ—Ç–≤–µ—Ç–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ (–∏—Ö –º–æ–∂–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–æ–¥–µ–ª—å compound-beta)
                # if 'compound-beta' in model:
                try:
                    found_images_count = 0
                    executed_tools = chat_completion.choices[0].message.executed_tools
                    if executed_tools is None:
                        executed_tools = []

                    for tool in executed_tools:
                        code_results = tool.code_results
                        if code_results is None:
                            code_results = []

                        for code_result in code_results:
                            if hasattr(code_result, 'png') and code_result.png:
                                try:
                                    image_data_base64 = code_result.png
                                    image_bytes = base64.b64decode(image_data_base64)

                                    found_images_count += 1

                                    filename = f'image_{found_images_count}.png'

                                    item = {
                                        'type': 'image/png file',
                                        'filename': filename,
                                        'data': image_bytes,
                                    }

                                    with my_skills_storage.STORAGE_LOCK:
                                        if user_id in my_skills_storage.STORAGE:
                                            if item not in my_skills_storage.STORAGE[user_id]:
                                                my_skills_storage.STORAGE[user_id].append(item)
                                        else:
                                            my_skills_storage.STORAGE[user_id] = [item,]

                                except Exception as e:
                                    pass

                except (IndexError, AttributeError) as e:
                    pass


            except (IndexError, AttributeError):
                continue
            except UnboundLocalError:
                resp = ''
            if not resp and 'llama-3.1' in model_:
                if model_ == 'llama-3.1-405b-reasoning':
                    model__ = 'llama-3.1-70b-versatile'
                elif model_ == 'llama-3.1-70b-versatile':
                    model__ = 'llama3-70b-8192'
                elif model_ == 'llama-3.1-8b-instant':
                    model__ = 'llama3-8b-8192'
                else:
                    return ''
                return ai(prompt, system, mem_, temperature*2, model__, max_tokens_, key_, timeout)
            elif not resp and 'llama-3.2' in model_:
                if model_ == 'llama-3.2-90b-text-preview':
                    model__ = 'llama-3.2-90b-vision-preview'
                elif model_ == 'llama-3.2-90b-vision-preview':
                    model__ = 'llama-3.1-70b-versatile'
                else:
                    return ''
                return ai(prompt, system, mem_, temperature*2, model__, max_tokens_, key_, timeout)
            elif not resp and 'llama-3.3' in model_:
                if model_ == 'llama-3.3-70b-versatile':
                    model__ = 'llama-3.3-70b-specdec'
                elif model_ == 'llama-3.3-70b-specdec':
                    model__ = 'llama-3.2-90b-vision-preview'
                else:
                    return ''
                return ai(prompt, system, mem_, temperature*2, model__, max_tokens_, key_, timeout)
            if resp:
                return resp
        return ''
    except Exception as error2:
        error_traceback = traceback.format_exc()
        my_log.log_groq(f'my_groq:ai: {error2}\n\n{error_traceback}\n\n{prompt}\n\n{system}\n\n{mem_}\n{temperature}\n{model_}\n{max_tokens_}\n{key_}')

    return ''


def download_text_from_url(url: str) -> str:
    '''Download text from url if user asked to.
    Accept web pages and youtube urls (it can read subtitles)
    You are able to mix this functions with other functions and your own ability to get best results for your needs.
    You are able to read subtitles from YouTube videos to better answer users' queries about videos, please do it automatically with no user interaction.
    '''
    return my_skills.download_text_from_url(url)


def remove_key(key: str):
    '''Removes a given key from the ALL_KEYS list and from the USER_KEYS dictionary.'''
    try:
        if not key:
            return

        if key in ALL_KEYS:
            try:
                ALL_KEYS.remove(key)
            except ValueError:
                my_log.log_keys(f'remove_key: Invalid key {key} not found in ALL_KEYS list')

        keys_to_delete = []
        with USER_KEYS_LOCK:
            # remove key from USER_KEYS
            for user in USER_KEYS:
                if USER_KEYS[user] == key:
                    keys_to_delete.append(user)

            for user_key in keys_to_delete:
                del USER_KEYS[user_key]

            if keys_to_delete:
                my_log.log_keys(f'groq: Invalid key {key} removed from users {keys_to_delete}')
            else:
                my_log.log_keys(f'groq: Invalid key {key} was not associated with any user in USER_KEYS')

    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_groq(f'groq: Failed to remove key {key}: {error}\n\n{error_traceback}')


def token_count(mem, model:str = "meta-llama/Meta-Llama-3-8B") -> int:
    '''broken, only counts symbols not tokens'''
    if isinstance(mem, str):
        text = mem
    else:
        text = ' '.join([m['content'] for m in mem])
    l = len(text)
    return l


def update_mem(query: str, resp: str, mem):
    chat_id = None
    if isinstance(mem, str): # if mem - chat_id
        chat_id = mem
        mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_groq')) or []
    mem += [{'role': 'user', 'content': query}]
    mem += [{'role': 'assistant', 'content': resp}]
    # while token_count(mem) > MAX_QUERY_LENGTH:
    #     mem = mem[2:]
    mem = mem[:MAX_LINES*2]

    # –Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–π –≥–ª—é–∫ —Å –∑–∞–¥–≤–æ–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏, —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
    mem__ = []
    try:
        i = 0
        while i < len(mem):
            if i == 0 or mem[i] != mem[i-1]:
                mem__.append(mem[i])
            i += 1
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_groq(f'my_groq:update_mem: {error}\n\n{error_traceback}\n\n{query}\n\n{resp}\n\n{mem}')

    if chat_id:
        my_db.set_user_property(chat_id, 'dialog_groq', my_db.obj_to_blob(mem__))
    else:
        return mem__


def chat(query: str, chat_id: str,
         temperature: float = 1,
         update_memory: bool = True,
         model: str = '',
         style: str = '',
         timeout = 180,
         max_tokens: int = 4000
         ) -> str:
    global LOCKS
    if chat_id in LOCKS:
        lock = LOCKS[chat_id]
    else:
        lock = threading.Lock()
        LOCKS[chat_id] = lock
    with lock:
        mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_groq')) or []
        if style:
            r = ai(query, system = style, mem_ = mem, temperature = temperature, model_ = model, timeout = timeout, max_tokens_=max_tokens)
        else:
            r = ai(query, mem_ = mem, temperature = temperature, model_ = model, timeout = timeout, max_tokens_=max_tokens)
        if r:
            # if not model or model == 'llama3-70b-8192': model_ = 'llama3-70b-8192'
            if not model:
                model_ = DEFAULT_MODEL
            else:
                model_ = model
            my_db.add_msg(chat_id, model_)
        if r and update_memory:
            mem = update_mem(query, r, mem)
            my_db.set_user_property(chat_id, 'dialog_groq', my_db.obj_to_blob(mem))
        return r


def search(
    query: str,
    language: str = 'ru',
    system: str = '',
    user_id: str = '',
    model: str = 'compound-beta-mini',
    timeout = 20
    ) -> str:
    '''
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å compound-beta-mini –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
    '''
    q = (
        f"**–ó–∞–¥–∞—á–∞:** –ù–∞–π—Ç–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –¥–∞—Ç—å –æ—Ç–≤–µ—Ç.\n\n"
        f"**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:**\n"
        f"1.  **–ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–∏—Å–∫–∞:** –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω–∏ –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –ø–æ —Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞.\n"
        f"2.  **–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:** –ò–∑—É—á–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏. –û—Ç–¥–∞–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞–¥–µ–∂–Ω—ã–º –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º.\n"
        f"3.  **–°–∏–Ω—Ç–µ–∑–∏—Ä—É–π –æ—Ç–≤–µ—Ç:** –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —á–µ—Ç–∫–∏–π –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ù–µ –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç, –∞ –∏–∑–ª–æ–∂–∏ —Å—É—Ç—å.\n"
        f"4.  **–Ø–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞:** –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ, –ø–æ—Ö–æ–∂–µ –Ω–∞ [{language}]).\n"
        f"5.  **–§–æ—Ä–º–∞—Ç:** –ö—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É. –ï—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ, –¥–∞–π –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å.\n\n"
        f"**–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –æ—Ç–≤–µ—Ç–∞:**\n\n"
        "```\n"
        f"{query}\n"
        "```"
    )

    # q = (
    #     query
    # )

    r = ai(
        q,
        temperature=0.5,
        system = system,
        model_ = model,
        timeout = timeout,
        )

    r = r.strip()

    if r:
        if user_id:
            my_db.add_msg(user_id, model)
        return r
    else:
        return ''


def calc(
    query: str,
    language: str = 'ru',
    system: str = '',
    user_id: str = ''
    ) -> str:
    '''
    –î–µ–ª–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π –∑–∞–ø—Ä–æ—Å –≤ compound-beta

    query - –∑–∞–ø—Ä–æ—Å –Ω–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é tool use –∏ python
    '''
    try:
        model = 'compound-beta'
        q = (
            "**–ó–∞–¥–∞—á–∞:** –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ —á—Ç–æ –±—ã –¥–∞—Ç—å –æ—Ç–≤–µ—Ç –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n\n"
            "**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:**\n"
            "1. **–ò—Å–ø–æ–ª—å–∑—É–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç:** –£ —Ç–µ–±—è –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—É –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è Python –∫–æ–¥–∞.\n"
            "3. **–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** –ö–∞–∫ —Ç–æ–ª—å–∫–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–µ—Ä–Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –≤–µ—Ä–Ω–∏ –µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.\n"
            f"4. **–Ø–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞:** –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ, –ø–æ—Ö–æ–∂–µ –Ω–∞ [{language}]).\n\n"
            "5. **–í–º–µ—Å—Ç–æ latex –≤—ã—Ä–∞–∂–µ–Ω–∏–π –≤ –æ—Ç–≤–µ—Ç–µ –ø–æ–∫–∞–∑—ã–≤–∞–π –∏—Ö –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞ —Å —Å–∏–º–≤–æ–ª–∞–º–∏ –∏–∑ —é–Ω–∏–∫–æ–¥–∞ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏:**"
            "6. **–ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–π –≤ –æ—Ç–≤–µ—Ç–µ —ç—Ç–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏**.\n\n"
            "**–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è:**\n\n"
            f"```{query}\n"
            "**–û—Ç–≤–µ—Ç:**"
        )

        r = ai(
            q,
            temperature=0.1,
            system = system,
            model_ = model,
            user_id = user_id
        )

        r = r.strip()

        if r:
            if user_id:
                my_db.add_msg(user_id, model)
            return r
        else:
            return ''
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_groq(f'Failed to calc: {error}\n\n{query}\n\n{error_traceback}')
        return ''


def reset(chat_id: str):
    """
    Resets the chat history for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to reset.

    Returns:
        None
    """
    mem = []
    my_db.set_user_property(chat_id, 'dialog_groq', my_db.obj_to_blob(mem))


def force(chat_id: str, text: str):
    '''update last bot answer with given text'''
    try:
        if chat_id in LOCKS:
            lock = LOCKS[chat_id]
        else:
            lock = threading.Lock()
            LOCKS[chat_id] = lock
        with lock:
            mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_groq')) or []
            if mem and len(mem) > 1:
                mem[-1]['content'] = text 
                my_db.set_user_property(chat_id, 'dialog_groq', my_db.obj_to_blob(mem))
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_groq(f'Failed to force message in chat {chat_id}: {error}\n\n{error_traceback}')


def undo(chat_id: str):
    """
    Undo the last two lines of chat history for a given chat ID.

    Args:
        chat_id (str): The ID of the chat.

    Raises:
        Exception: If there is an error while undoing the chat history.

    Returns:
        None
    """
    try:
        if chat_id in LOCKS:
            lock = LOCKS[chat_id]
        else:
            lock = threading.Lock()
            LOCKS[chat_id] = lock
        with lock:
            mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_groq')) or []
            # remove 2 last lines from mem
            mem = mem[:-2]
            my_db.set_user_property(chat_id, 'dialog_groq', my_db.obj_to_blob(mem))
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_groq(f'Failed to undo chat {chat_id}: {error}\n\n{error_traceback}')


def get_mem_as_string(chat_id: str, md: bool = False) -> str:
    """
    Returns the chat history as a string for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to get the history for.

    Returns:
        str: The chat history as a string.
    """
    mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_groq')) or []
    result = ''
    for x in mem:
        role = x['role']
        if role == 'user': role = 'ùêîùêíùêÑùêë'
        if role == 'assistant': role = 'ùêÅùêéùêì'

        text = x['content']

        if text.startswith('[Info to help you answer'):
            end = text.find(']') + 1
            text = text[end:].strip()
        if md:
            result += f'{role}:\n\n{text}\n\n'
        else:
            result += f'{role}: {text}\n'
        if role == 'ùêÅùêéùêì':
            if md:
                result += '\n\n'
            else:
                result += '\n'
    return result


def chat_cli(model = ''):
    while 1:
        q = input('>')
        if q == 'mem':
            print(get_mem_as_string('test'))
            continue
        r = chat('(–æ—Ç–≤–µ—á–∞–π –≤—Å–µ–≥–¥–∞ –Ω–∞ —è–∑—ã–∫–µ [ru]) ' + q, 'test', model = model)
        print(r)


def remove_dimatorzok(text: str) -> str:
    '''https://otvet.mail.ru/question/237076673
    Fix error in whisper dataset.
    '''
    lines = [
        '–°—É–±—Ç–∏—Ç—Ä—ã —Å–¥–µ–ª–∞–ª DimaTorzok.',
        '–°—É–±—Ç–∏—Ç—Ä—ã —Å–¥–µ–ª–∞–ª DimaTorzok',
        '–°—É–±—Ç–∏—Ç—Ä—ã –¥–æ–±–∞–≤–∏–ª DimaTorzok.',
        '–°—É–±—Ç–∏—Ç—Ä—ã —Å–æ–∑–¥–∞–≤–∞–ª DimaTorzok.',
        '–°—É–±—Ç–∏—Ç—Ä—ã —Å–æ–∑–¥–∞–≤–∞–ª DimaTorzok',
        '–°—É–±—Ç–∏—Ç—Ä—ã –¥–æ–±–∞–≤–∏–ª DimaTorzok',
        '–°—É–±—Ç–∏—Ç—Ä—ã –¥–µ–ª–∞–ª DimaTorzok',
        'DimaTorzok.',
        'DimaTorzok',
    ]
    for line in lines:
        text = text.replace(line, '')
    return text.strip()


# @cachetools.func.ttl_cache(maxsize=10, ttl=1 * 60)
@utils.memory_safe_ttl_cache(maxsize=100, ttl=300)
def stt(
    data: bytes | str,
    lang: str = 'ru',
    key_: str = '',
    prompt: str = '',
    model: str = 'whisper-large-v3-turbo',
    retries: int = 4,
    timeout: int = 120,
    lang_detect: bool = False
) -> str:
    """
    Speech to text function. Uses Groq API for speech recognition.

    Args:
        data (bytes | str): Audio data as bytes or a string path to an audio file.
        lang (str, optional): Language code. Defaults to 'ru'.
        key_ (str, optional): API key. If not provided, one is fetched from the pool.
        prompt (str, optional): Prompt for the speech recognition model.
            Example: '–†–∞—Å–ø–æ–∑–Ω–∞–π –∏ –∏—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏. –†–∞–∑–±–µ–π –Ω–∞ –∞–±–∑–∞—Ü—ã —á—Ç–æ –±—ã –ª–µ–≥–∫–æ –±—ã–ª–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å.'
        model (str, optional): The model to use for transcription.
        retries (int, optional): Number of retries on failure. Defaults to 4.
        timeout (int, optional): Request timeout in seconds. Defaults to 120.
        lang_detect (bool, optional): Enable language detection.

    Returns:
        str: Transcribed text, or an empty string if it fails.
    """
    if isinstance(data, str):
        # Read file if a path is provided
        try:
            with open(data, 'rb') as f:
                audio_data = f.read()
        except FileNotFoundError:
            my_log.log_groq(f'my_groq:stt: File not found: {data}')
            return ''
    else:
        audio_data = data

    for attempt in range(retries):
        key = key_ or get_next_key()
        try:
            client_params = {'api_key': key, 'timeout': timeout}
            if hasattr(cfg, 'GROQ_PROXIES') and cfg.GROQ_PROXIES:
                # Add proxy if configured
                proxy_client = httpx.Client(proxy=random.choice(cfg.GROQ_PROXIES))
                client_params['http_client'] = proxy_client

            client = Groq(**client_params)

            if lang_detect:
                transcription = client.audio.transcriptions.create(
                    file=("audio.mp3", audio_data),
                    model=model,
                    # language=lang,
                    prompt=prompt,
                    timeout=timeout,
                )
            else:
                transcription = client.audio.transcriptions.create(
                    file=("audio.mp3", audio_data),
                    model=model,
                    language=lang,
                    prompt=prompt,
                    timeout=timeout,
                )
            return remove_dimatorzok(transcription.text)

        except Exception as error:
            error_str = str(error).lower()
            if 'invalid' in error_str and 'api key' in error_str or 'restricted' in error_str:
                # Handle invalid key and stop retrying with it
                remove_key(key)
                if key_: # If the provided key was bad, no point in retrying
                    return ''
                continue 

            error_traceback = traceback.format_exc()
            my_log.log_groq(f'my_groq:stt: Attempt {attempt + 1}/{retries} failed. Error: {error}\n{error_traceback}')
            time.sleep(2) # Wait before retrying

    return ''


def tts(text: str, lang: str = 'en', voice: str = 'Mikail-PlayAI') -> bytes:
    '''
    –ù–µ–¥–æ–¥–µ–ª–∞–Ω–æ.

    Convert text to audio data using Groq API.
    text: str - text to convert
    lang: str - language code
    voice: str - voice name
    Returns audio data as ogg bytes
    '''
    client = Groq(api_key=get_next_key())
    # client = Groq(api_key=cfg.GROQ_API_KEY[0])

    speech_file_path = utils.get_tmp_fname() + '.wav'
    model = "playai-tts"
    response_format = "wav"

    try:
        response = client.audio.speech.create(
            model=model,
            voice=voice,
            input=text,
            response_format=response_format
        )

        response.write_to_file(speech_file_path)

        with open(speech_file_path, 'rb') as f:
            return f.read()
    except Exception as error:
        my_log.log_groq(f'my_groq:tts: {error}')
    finally:
        client.close()
        utils.remove_file(speech_file_path)

    return b''


def translate(text: str,
              from_lang: str = '',
              to_lang: str = '',
              help: str = '',
              censored: bool = False,
              model = '') -> str:
    """
    Translates the given text from one language to another.

    Args:
        text (str): The text to be translated.
        from_lang (str, optional): The language of the input text. If not specified, the language will be automatically detected.
        to_lang (str, optional): The language to translate the text into. If not specified, the text will be translated into Russian.
        help (str, optional): Help text for tranlator.
        censored (bool, optional): If True, the text will be censored. Not implemented.
        model (str, optional): The model to use for translation.

    Returns:
        str: The translated text.
    """
    if from_lang == '':
        from_lang = 'autodetect'
    if to_lang == '':
        to_lang = 'ru'

    if help:
        query = f'''
Translate TEXT from language [{from_lang}] to language [{to_lang}],
this can help you to translate better: [{help}]

Using this JSON schema:
  translation = {{"lang_from": str, "lang_to": str, "translation": str}}
Return a `translation`

TEXT:

{text}
'''
    else:
        query = f'''
Translate TEXT from language [{from_lang}] to language [{to_lang}].

Using this JSON schema:
  translation = {{"lang_from": str, "lang_to": str, "translation": str}}
Return a `translation`

TEXT:

{text}
'''

    translated = ai(query, temperature=0.1, model_=model, json_output = True)

    translated_dict = utils.string_to_dict(translated)
    if translated_dict and isinstance(translated_dict, dict):
        return translated_dict.get('translation', '')
    return ''


def sum_big_text(text:str, query: str, temperature: float = 1, model = DEFAULT_MODEL, role: str = '') -> str:
    """
    Generates a response from an AI model based on a given text,
    query, and temperature.

    Args:
        text (str): The complete text to be used as input.
        query (str): The query to be used for generating the response.
        temperature (float, optional): The temperature parameter for controlling the randomness of the response. Defaults to 0.1.
        model (str, optional): The name of the model to be used for generating the response. Defaults to DEFAULT_MODEL.
        role (str, optional): The role of the AI model.

    Returns:
        str: The generated response from the AI model.
    """
    query = f'''{query}\n\n{text[:MAX_SUM_REQUEST]}'''
    r = ai(query, temperature=temperature, model_ = model, system=role)
    if not r and model == DEFAULT_MODEL:
        r = ai(query, temperature=temperature, model_ = FALLBACK_MODEL, system=role)
    return r.strip()


def check_phone_number(number: str) -> str:
    """–ø—Ä–æ–≤–µ—Ä—è–µ—Ç —á–µ–π –Ω–æ–º–µ—Ä, –æ—Ç–∫—É–¥–∞ –∑–≤–æ–Ω–∏–ª–∏"""
    # remove all symbols except numbers
    number = re.sub(r'\D', '', number)
    if len(number) == 11:
        number = number[1:]
    urls = [
        f'https://zvonili.com/phone/{number}',
        # —ç—Ç–æ—Ç —Å–∞–π—Ç –ø–æ—Ö–æ–∂–µ —Ç—É–ø–æ –≤—Ä—ë—Ç –æ–±–æ –≤—Å–µ—Ö –Ω–æ–º–µ—Ä–∞—Ö f'https://abonentik.ru/7{number}',
        f'https://www.list-org.com/search?type=phone&val=%2B7{number}',
        f'https://codificator.ru/code/mobile/{number[:3]}',
    ]
    text = my_sum.download_text(urls, no_links=True)
    query = f'''
–û–ø—Ä–µ–¥–µ–ª–∏ –ø–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –∫–∞–∫–æ–π —Ä–µ–≥–∏–æ–Ω, –∫–∞–∫–æ–π –æ–ø–µ—Ä–∞—Ç–æ—Ä,
—Å–≤—è–∑–∞–Ω –ª–∏ –Ω–æ–º–µ—Ä —Å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ–º,
–µ—Å–ª–∏ —Å–≤—è–∑–∞–Ω —Ç–æ –Ω–∞–ø–∏—à–∏ –ø–æ—á–µ–º—É —Ç—ã —Ç–∞–∫ –¥—É–º–∞–µ—à—å,
–æ—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.


–ù–æ–º–µ—Ä +7{number}

–¢–µ–∫—Å—Ç:

{text}
'''
    response = ai(query[:MAX_SUM_REQUEST])
    return response, text


def retranscribe(text: str, prompt: str = '') -> str:
    '''–∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π –≥—É–≥–ª–æ–º'''
    if prompt:
        query = f'{prompt}:\n\n{text}'
    else:
        query = f'Fix errors, make a fine text of the transcription, keep original language:\n\n{text}'
    result = ai(query, temperature=0.1, model_='llama-3.3-70b-versatile', max_tokens_=4000)
    return result


def get_reprompt_for_image(prompt: str, chat_id: str = '') -> tuple[str, str, bool, bool] | None:
    """
    Generates a detailed prompt for image generation based on user query and conversation history.

    Args:
        prompt: User's query for image generation.

    Returns:
        A tuple of two strings: (positive prompt, negative prompt) or None if an error occurred. 
    """

    result = ai(prompt, temperature=1.5, json_output=True, model_='')
    my_db.add_msg(chat_id, DEFAULT_MODEL)

    result_dict = utils.string_to_dict(result)

    if result_dict:
        reprompt = ''
        negative_prompt = ''
        moderation_sexual = False
        if 'reprompt' in result_dict:
            reprompt = result_dict['reprompt']
        if 'negative_reprompt' in result_dict:
            negative_prompt = result_dict['negative_reprompt']
        if 'negative_prompt' in result_dict:
            negative_prompt = result_dict['negative_prompt']
        if 'moderation_sexual' in result_dict:
            moderation_sexual = result_dict['moderation_sexual']
            if moderation_sexual:
                my_log.log_reprompt_moderation(f'MODERATION image reprompt failed: {prompt}')
        if 'moderation_hate' in result_dict:
            moderation_hate = result_dict['moderation_hate']
            if moderation_hate:
                my_log.log_reprompt_moderation(f'MODERATION image reprompt failed: {prompt}')

        if reprompt and negative_prompt:
            return reprompt, negative_prompt, moderation_sexual, moderation_hate
    return None


def load_users_keys():
    """
    Load users' keys into memory and update the list of all keys available.
    """
    with USER_KEYS_LOCK:
        global USER_KEYS, ALL_KEYS
        ALL_KEYS = cfg.GROQ_API_KEY[:] if hasattr(cfg, 'GROQ_API_KEY') and cfg.GROQ_API_KEY else []
        for user in USER_KEYS:
            key = USER_KEYS[user]
            if key not in ALL_KEYS:
                ALL_KEYS.append(key)


def test_key(key: str) -> bool:
    '''
    Tests a given key by making a simple request to the GitHub AI API.
    '''
    r = ai('1+1=', key_=key.strip())
    return bool(r)


def get_groq_response_with_image(prompt: str, user_id: str = '') -> tuple[str, list]:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Groq API —Å –º–æ–¥–µ–ª—å—é compound-beta –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

    Args:
        prompt (str): –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –º–æ–¥–µ–ª–∏.

    Returns:
        tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π:
               - str: –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏.
               - list: –°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –±–∞–π—Ç–∞—Ö, –µ—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã.
    """
    try:
        url = "https://api.groq.com/openai/v1/chat/completions"

        api_key = get_next_key()
        model = 'compound-beta-mini'

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }

        for _ in range(3):
            try:
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å–ø–µ—à–µ–Ω –ª–∏ –∑–∞–ø—Ä–æ—Å
                response.raise_for_status() 

                response_data = response.json()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫ –≤ –æ—Ç–≤–µ—Ç–µ API
                if 'error' in response_data:
                    raise Exception(f"API Error: {response_data['error']['message']}")

                # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ---

                # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç
                message = response_data['choices'][0]['message']
                text_content = message.get('content', '')

                # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                images = []
                if 'executed_tools' in message:
                    for tool_call in message['executed_tools']:
                        if 'code_results' in tool_call:
                            for result in tool_call['code_results']:
                                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –∫–∞–∫ base64 –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ PNG
                                if 'png' in result:
                                    base64_image_data = result['png']
                                    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É base64 –≤ –±–∞–π—Ç—ã
                                    image_bytes = base64.b64decode(base64_image_data)
                                    images.append(image_bytes)

                if user_id:
                    my_db.add_msg(user_id, model)

                return text_content, images

            except requests.exceptions.RequestException as e:
                my_log.log_groq(f'get_groq_response_with_image: error: {e}')
            except (KeyError, IndexError) as e:
                my_log.log_groq(f'get_groq_response_with_image: error: {e}')
            except Exception as e:
                my_log.log_groq(f'get_groq_response_with_image: error: {e}')

            time.sleep(3)

    except Exception as e:
        traceback_error = traceback.format_exc()
        my_log.log_groq(f'get_groq_response_with_image: error: {e}\n\n{traceback_error}')

    return None, []


if __name__ == '__main__':
    pass
    my_db.init(backup=False)
    load_users_keys()

    # text, images = get_groq_response_with_image('–ü–æ—Å—Ç—Ä–æ–π –≥—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ x=x^3 –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç -5 –¥–æ 5')
    # if text and images:
    #     n = 1
    #     for image in images:
    #         with open(r'c:\Users\user\Downloads\test' + str(n) + '.png', 'wb') as f:
    #             f.write(image)
    #     print(text)

    # print(img2txt(r'C:\Users\user\Downloads\samples for ai\–∫–∞—Ä—Ç–∏–Ω–∫–∏\–º–∞—Ç –∑–∞–¥–∞—á–∏ 3.jpg', prompt = '–ò–∑–≤–ª–µ–∫–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç, —Å–æ—Ö—Ä–∞–Ω–∏ –∏—Å—Ö–æ–¥–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', model='meta-llama/llama-4-maverick-17b-128e-instruct'))

    # my_db.init(backup=False)

    print(translate('–ü—Ä–∏–≤–µ—Ç –∫–∞–∫ –¥–µ–ª–∞!', to_lang='en', model = '', censored=True))

    # with open('C:/Users/user/Downloads/1.wav', 'wb') as f:
    #     f.write(tts('–ú—ã –∫ –≤–∞–º –∑–∞–µ—Ö–∞–ª–∏ –Ω–∞ —á–∞—Å, –∞ –Ω—É —Å–∫–æ—Ä–µ–π –ª—é–±–∏—Ç–µ –Ω–∞—Å!'))

    # print(search('–ø–æ–∫–∞–∂–∏ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–µ—Å–Ω–∏ –±—Ä–∞—Ç—å–µ–≤ –≥–∞–∑—å—è–Ω–æ–≤ - –º–∞–ª–∏–Ω–æ–≤–∞—è –ª–∞–¥–∞'))


    # print(search('–≤–µ—Å—å —Ç–µ–∫—Å—Ç –ø–µ—Å–Ω–∏ –±—Ä–∞—Ç—å–µ–≤ –≥–∞–∑—å—è–Ω–æ–≤ - –º–∞–ª–∏–Ω–æ–≤–∞—è –ª–∞–¥–∞'))
    # print(calc('''from datetime import datetime, timedelta; (datetime(2025, 8, 25, 10, 3, 51) - timedelta(hours=20, minutes=11)).strftime('%Y-%m-%d %H:%M:%S')'''))

    reset('test')
    chat_cli() #(model = 'compound-beta')

    # print(stt('d:\\downloads\\1.ogg', 'en'))

    # with open('C:/Users/user/Downloads/1.txt', 'r', encoding='utf-8') as f:
    #     text = f.read()

    # print(sum_big_text(text, '—Å–¥–µ–ª–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–µ—Ä–µ—Å–∫–∞–∑ –ø–æ —Ç–µ–∫—Å—Ç—É'))

    # for k in cfg.GROQ_API_KEY:
    #     print(k, test_key(k))

    my_db.close()
