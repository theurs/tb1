#!/usr/bin/env python3

import base64
import binascii
import json
import random
import requests
import time
import threading
import traceback
from typing import Optional, List, Union, Dict, Any

from sqlitedict import SqliteDict

import cfg
import my_db
import my_log
import utils


# —Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ —Ö—Ä–∞–Ω–∏—Ç—å
MAX_MEM_LINES = 20
MAX_HIST_CHARS = 100000

# –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —á–∞—Ç–æ–≤ —á—Ç–æ –±—ã –Ω–µ –∏—Å–ø–æ—Ä—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é
# {id:lock}
LOCKS = {}

# –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã –±–æ–ª—å—à–µ —á–µ–º, —ç—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç–∞, –≤ —ç—Ç–æ–º –º–æ–¥—É–ª–µ –æ–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
MAX_REQUEST = 50000

DEFAULT_MODEL = 'qwen/qwen3-235b-a22b-07-25:free'
DEFAULT_MODEL_FALLBACK = 'qwen/qwen3-235b-a22b:free'
GEMINI25_FLASH_IMAGE = 'google/gemini-2.5-flash-image-preview:free'


# —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –∫–ª—é—á–µ–π {key: unfreeze_timestamp}
FROZEN_KEYS = SqliteDict('db/openrouter_frozen_keys.db', autocommit=True)
FROZEN_KEYS_LOCK = threading.Lock()
CURRENT_KEY_INDEX = 0
KEY_INDEX_LOCK = threading.Lock()


def freeze_key(key: str, duration_seconds: int = 86400) -> None:
    """
    Temporarily freezes a rate-limited API key by adding it to the FROZEN_KEYS database.

    Args:
        key (str): The API key to freeze.
        duration_seconds (int): The duration in seconds for which the key will be frozen. Defaults to 86400 (24 hours).
    """
    with FROZEN_KEYS_LOCK:
        unfreeze_time = time.time() + duration_seconds
        FROZEN_KEYS[key] = unfreeze_time
        my_log.log_openrouter_free(f'Key frozen due to rate limit until {time.ctime(unfreeze_time)}: ...{key[-4:]}')


def get_available_key() -> Optional[str]:
    """
    Retrieves the next available, non-frozen API key using a round-robin strategy
    and cleans up expired frozen keys.

    Returns:
        Optional[str]: An available API key, or None if no keys are available.
    """
    global CURRENT_KEY_INDEX
    with FROZEN_KEYS_LOCK:
        # Clean up expired keys from the frozen list
        current_time = time.time()
        expired_keys = [key for key, unfreeze_time in FROZEN_KEYS.items() if current_time > unfreeze_time]
        for key in expired_keys:
            del FROZEN_KEYS[key]
            my_log.log_openrouter_free(f'Key unfrozen: ...{key[-4:]}')

        # Get the list of all keys and filter out the currently frozen ones
        all_keys = cfg.OPEN_ROUTER_FREE_KEYS if hasattr(cfg, 'OPEN_ROUTER_FREE_KEYS') else []
        frozen_keys_set = set(FROZEN_KEYS.keys())
        available_keys = [key for key in all_keys if key not in frozen_keys_set]

    if not available_keys:
        my_log.log_openrouter_free('No available (non-frozen) API keys.')
        return None

    # Select the next key using round-robin logic in a thread-safe manner
    with KEY_INDEX_LOCK:
        # Ensure the index wraps around the current list of available keys
        index = CURRENT_KEY_INDEX % len(available_keys)
        key_to_return = available_keys[index]
        # Move to the next index for the subsequent call
        CURRENT_KEY_INDEX += 1

    return key_to_return


def clear_mem(mem, user_id: str):
    while 1:
        sizeofmem = count_tokens(mem)
        if sizeofmem <= MAX_HIST_CHARS:
            break
        try:
            mem = mem[2:]
        except IndexError:
            mem = []
            break

    return mem[-MAX_MEM_LINES*2:]


def count_tokens(mem) -> int:
    return sum([len(m['content']) for m in mem])


def ai(prompt: str = '',
       mem = None,
       user_id: str = '',
       system: str = '',
       model = DEFAULT_MODEL,
       temperature: float = 1,
       max_tokens: int = 4000,
       timeout: int = 120) -> str:

    if not model:
        model = DEFAULT_MODEL

    if not model.endswith(':free'):
        return ''

    if not prompt and not mem:
        return ''

    if not hasattr(cfg, 'OPEN_ROUTER_FREE_KEYS') or len(cfg.OPEN_ROUTER_FREE_KEYS) < 1:
        return ''

    if not temperature:
        temperature = 0.1
    if 'llama' in model and temperature > 0:
        temperature = temperature / 2

    mem_ = mem[:] if mem else []

    # current date time string
    now = utils.get_full_time()
    systems = (
        f'Current date and time: {now}\n',
        'Ask again if something is unclear in the request',
        'You (assistant) are currently working in a Telegram bot. The Telegram bot automatically extracts text from any type of files sent to you by the user, such as documents, images, audio recordings, etc., so that you can fully work with any files.',
        # 'You (assistant) can use some telegram bot functions, answer starting /tts lang text-to-say (example: /tts ru –ü—Ä–∏–≤–µ—Ç) and user will get voice message from you, answer starting /img detailed-professional-description-of-an-image and user will get images generated by ai.',
        "If the user's request cannot be fulfilled using the available tools or direct actions, the assistant(you) must treat the request as a request to generate text (e.g., providing code as text), not a request to perform an action (e.g., executing code or interacting with external systems not directly supported by tools) (intention mismatch).",
        "To edit image user can send image with caption starting ! symbol",
    )

    if system:
        mem_.insert(0, {"role": "system", "content": system})
    for s in reversed(systems):
        mem_.insert(0, {"role": "system", "content": s})
    if prompt:
        mem_ = mem_ + [{'role': 'user', 'content': prompt}]

    YOUR_SITE_URL = 'https://t.me/kun4sun_bot'
    YOUR_APP_NAME = 'kun4sun_bot'

    result = ''

    start_time = time.time()

    for _ in range(3):
        if time.time() - start_time > timeout:
            return ''

        key = get_available_key()
        if not key:
            time.sleep(5)  # Wait if no keys are available
            continue

        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {key}",
                "HTTP-Referer": f"{YOUR_SITE_URL}",
                "X-Title": f"{YOUR_APP_NAME}",
            },
            data=json.dumps({
                "model": model,
                "messages": mem_,
                "max_tokens": max_tokens,
                "temperature": temperature,
            }),
            timeout = timeout,
        )

        status = response.status_code
        if status == 200:
            try:
                result = response.json()['choices'][0]['message']['content'].strip()
                break
            except Exception as error:
                my_log.log_openrouter_free(f'Failed to parse response: {error}\n\n{str(response)}')
                result = ''
                if response.text.startswith("""{"error":{"message":"This endpoint\'s maximum context length is"""):
                    return ''
                if not result and model == DEFAULT_MODEL:
                    return ai(prompt, mem, user_id, system, DEFAULT_MODEL_FALLBACK, temperature, max_tokens, timeout)
                time.sleep(2)
        else:
            if status == 429 and "free-models-per-day" in response.text:
                freeze_key(key)
            my_log.log_openrouter_free(f'Bad response.status_code\n\n{str(response)[:2000]}')
            time.sleep(2)

    if not result and model == DEFAULT_MODEL:
        result = ai(prompt, mem, user_id, system, DEFAULT_MODEL_FALLBACK, temperature, max_tokens, timeout)

    return result


def update_mem(query: str, resp: str, chat_id: str):
    mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []
    mem += [{'role': 'user', 'content': query}]
    mem += [{'role': 'assistant', 'content': resp}]
    mem = clear_mem(mem, chat_id)

    mem__ = []
    try:
        i = 0
        while i < len(mem):
            if i == 0 or mem[i] != mem[i-1]:
                mem__.append(mem[i])
            i += 1
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_openrouter_free(f'my_openrouter:update_mem: {error}\n\n{error_traceback}\n\n{query}\n\n{resp}\n\n{mem}')

    my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem__))


def chat(
    query: str,
    chat_id: str = '',
    temperature: float = 1,
    system: str = '',
    model: str = ''
    ) -> str:
    global LOCKS
    if chat_id in LOCKS:
        lock = LOCKS[chat_id]
    else:
        lock = threading.Lock()
        LOCKS[chat_id] = lock
    with lock:
        mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []

        text = ai(query, mem, user_id=chat_id, temperature = temperature, system=system, model=model)

        if text:
            if DEFAULT_MODEL in model:
                my_db.add_msg(chat_id, 'llama-4-maverick')
            elif DEFAULT_MODEL_FALLBACK in model:
                my_db.add_msg(chat_id, 'llama-4-scout')
            mem += [{'role': 'user', 'content': query}]
            mem += [{'role': 'assistant', 'content': text}]
            mem = clear_mem(mem, chat_id)
            my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem))
        return text
    return ''


def chat_cli(model: str = ''):
    reset('test')
    while 1:
        q = input('>')
        if q == 'mem':
            print(get_mem_as_string('test'))
            continue
        r = chat(q, 'test', model = model, system='–æ—Ç–≤–µ—á–∞–π –≤—Å–µ–≥–¥–∞ –Ω–∞ —è–∑—ã–∫–µ')
        print(r)


def force(chat_id: str, text: str):
    '''update last bot answer with given text'''
    try:
        if chat_id in LOCKS:
            lock = LOCKS[chat_id]
        else:
            lock = threading.Lock()
            LOCKS[chat_id] = lock
        with lock:
            mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []
            if mem and len(mem) > 1:
                mem[-1]['content'] = text
                my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem))
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_openrouter_free(f'Failed to force message in chat {chat_id}: {error}\n\n{error_traceback}')


def undo(chat_id: str):
    """
    Undo the last two lines of chat history for a given chat ID.

    Args:
        chat_id (str): The ID of the chat.

    Raises:
        Exception: If there is an error while undoing the chat history.

    Returns:
        None
    """
    try:
        if chat_id in LOCKS:
            lock = LOCKS[chat_id]
        else:
            lock = threading.Lock()
            LOCKS[chat_id] = lock
        with lock:
            mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []
            # remove 2 last lines from mem
            mem = mem[:-2]
            my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem))
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_openrouter_free(f'Failed to undo chat {chat_id}: {error}\n\n{error_traceback}')


def reset(chat_id: str):
    """
    Resets the chat history for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to reset.

    Returns:
        None
    """
    mem = []
    my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem))


def get_mem_as_string(chat_id: str, md: bool = False) -> str:
    """
    Returns the chat history as a string for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to get the history for.

    Returns:
        str: The chat history as a string.
    """
    try:
        mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []
        result = ''
        for x in mem:
            role = x['role']
            if role == 'user': role = 'ùêîùêíùêÑùêë'
            if role == 'assistant': role = 'ùêÅùêéùêì'
            if role == 'system': role = 'ùêíùêòùêíùêìùêÑùêå'
            text = x['content']
            if text.startswith('[Info to help you answer'):
                end = text.find(']') + 1
                text = text[end:].strip()
            if md:
                result += f'{role}:\n\n{text}\n\n'
            else:
                result += f'{role}: {text}\n'
            if role == 'ùêÅùêéùêì':
                if md:
                    result += '\n\n'
                else:
                    result += '\n'
        return result 
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_openrouter_free(f'get_mem_as_string: {error}\n\n{error_traceback}')
        return ''


def img2txt(
    image_data: bytes,
    prompt: str = 'Describe picture',
    model = DEFAULT_MODEL,
    temperature: float = 1,
    max_tokens: int = 2000,
    timeout: int = 120,
    chat_id: str = '',
    system: str = '',
    ) -> str:
    """
    Describes an image using the specified model and parameters.

    Args:
        image_data: The image data as bytes.
        prompt: The prompt to guide the description. Defaults to 'Describe picture'.
        model: The model to use for generating the description. Defaults to DEFAULT_MODEL.
        temperature: The temperature parameter for controlling the randomness of the output. Defaults to 1.
        max_tokens: The maximum number of tokens to generate. Defaults to 2000.
        timeout: The timeout for the request in seconds. Defaults to 120.

    Returns:
        A string containing the description of the image, or an empty string if an error occurs.
    """

    if isinstance(image_data, str):
        with open(image_data, 'rb') as f:
            image_data = f.read()

    if not model:
        model = DEFAULT_MODEL

    if not model.endswith(':free'):
        return ''

    if not prompt:
        prompt = 'Describe picture'
        return ''

    if not hasattr(cfg, 'OPEN_ROUTER_FREE_KEYS'):
        return ''

    if 'llama' in model and temperature > 0:
        temperature = temperature / 2

    base64_image = base64.b64encode(image_data).decode()

    mem = [
        {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": prompt
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            }
        ]
        }
    ]
    if system:
        mem.insert(0, {'role': 'system', 'content': system})

    YOUR_SITE_URL = 'https://t.me/kun4sun_bot'
    YOUR_APP_NAME = 'kun4sun_bot'

    result = ''

    for _ in range(3):
        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {random.choice(cfg.OPEN_ROUTER_FREE_KEYS)}",
                "HTTP-Referer": f"{YOUR_SITE_URL}",  # Optional, for including your app on openrouter.ai rankings.
                "X-Title": f"{YOUR_APP_NAME}",  # Optional. Shows in rankings on openrouter.ai.
            },
            data=json.dumps({

                "model": model,
                "temperature": temperature,
                "messages": mem,
                "max_tokens": max_tokens

            }),
            timeout=timeout,
        )

        status = response.status_code
        if status == 200:
            try:
                result = response.json()['choices'][0]['message']['content'].strip()
                break
            except Exception as error:
                my_log.log_openrouter_free(f'Failed to parse response: {error}\n\n{str(response)}')
                result = ''
                time.sleep(2)
        else:
            my_log.log_openrouter_free(f'Bad response.status_code\n\n{str(response)[:2000]}')
            time.sleep(2)
    if chat_id:
        my_db.add_msg(chat_id, model)

    if not result and model == DEFAULT_MODEL:
        result = img2txt(image_data, prompt, DEFAULT_MODEL_FALLBACK, temperature, max_tokens, timeout, chat_id, system)

    return result


# def voice2txt(
#     voice_data: bytes,
#     model = 'google/gemini-flash-8b-1.5-exp',
#     temperature: float = 0,
#     max_tokens: int = 2000,
#     timeout: int = 120) -> str:
#     """
#     Transcribes audio data to text using the specified model and parameters.

#     Args:
#         voice_data: The audio data as bytes.
#         model: The model to use for generating the transcription. Defaults to 'google/gemini-flash-8b-1.5-exp'.
#         temperature: The temperature parameter for controlling the randomness of the output. Defaults to 0.
#         max_tokens: The maximum number of tokens to generate. Defaults to 2000.
#         timeout: The timeout for the request in seconds. Defaults to 120.

#     Returns:
#         A string containing the transcribed text, or an empty string if an error occurs.
#     """

#     if isinstance(voice_data, str):
#         with open(voice_data, 'rb') as f:
#             voice_data = f.read()

#     if not model:
#         model = 'google/gemini-flash-8b-1.5-exp'

#     # if not model.endswith(':free'):
#     #     return ''

#     if not hasattr(cfg, 'OPEN_ROUTER_FREE_KEYS'):
#         return ''

#     base64_voice = base64.b64encode(voice_data).decode()

#     YOUR_SITE_URL = 'https://t.me/kun4sun_bot'
#     YOUR_APP_NAME = 'kun4sun_bot'

#     result = ''

#     for _ in range(3):
#         response = requests.post(
#             url="https://openrouter.ai/api/v1/chat/completions",
#             headers={
#                 "Authorization": f"Bearer {random.choice(cfg.OPEN_ROUTER_FREE_KEYS)}",
#                 "HTTP-Referer": f"{YOUR_SITE_URL}",  # Optional, for including your app on openrouter.ai rankings.
#                 "X-Title": f"{YOUR_APP_NAME}",  # Optional. Shows in rankings on openrouter.ai.
#             },
#             data=json.dumps({

#                 "model": model,
#                 "temperature": temperature,
#                 "messages": [
#                     {
#                     "role": "user",
#                     "content": [
#                         {
#                             "type": "text",
#                             "text": 'transcribe it'
#                         },
#                         {
#                             "type": "voice_url",
#                             "voice_url": {
#                                 "url": f"data:audio/mpeg;base64,{base64_voice}"
#                             }
#                         }
#                     ]
#                     }
#                 ],
#                 "max_tokens": max_tokens

#             }),
#             timeout=timeout,
#         )

#         status = response.status_code
#         if status == 200:
#             try:
#                 result = response.json()['text'].strip()
#                 break
#             except Exception as error:
#                 my_log.log_openrouter_free(f'Failed to parse response: {error}\n\n{str(response)}')
#                 result = ''
#                 time.sleep(2)
#         else:
#             my_log.log_openrouter_free(f'Bad response.status_code\n\n{str(response)[:2000]}')
#             time.sleep(2)

#     return result


#### flash25 image generate and edit ####


def _send_image_request(
    model: str,
    messages: List[Dict[str, Any]],
    user_id: str,
    timeout: int,
    log_context: str,
    temperature: float,
) -> Optional[bytes]:
    """
    Sends a request to the OpenRouter API for image generation or editing with a total timeout.

    This private helper function handles the entire request lifecycle, including
    authentication, payload construction, sending the request, retrying on
    server errors, and parsing the response to extract the image data, all
    within a cumulative timeout using a monotonic clock for reliability.

    Args:
        model (str): The model identifier to use for the request.
        messages (List[Dict[str, Any]]): The list of messages forming the conversation.
        user_id (str): The ID of the user initiating the request, for logging purposes.
        timeout (int): The total request timeout in seconds for all retries.
        log_context (str): A string identifier for the calling function (e.g., 'txt2img').
        temperature (float): The temperature for the generation.

    Returns:
        Optional[bytes]: The image data as bytes if the request is successful, otherwise None.
    """
    if not hasattr(cfg, 'OPEN_ROUTER_FREE_KEYS') or not cfg.OPEN_ROUTER_FREE_KEYS:
        return None

    YOUR_SITE_URL = 'https://t.me/kun4sun_bot'
    YOUR_APP_NAME = 'kun4sun_bot'

    payload = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
    }

    # Disable safety filters for Google Gemini models
    if "gemini" in model.lower():
        payload["safety_settings"] = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]

    start_time = time.monotonic()  # Use monotonic clock for reliable timeout measurement

    for attempt in range(3):  # Retry loop
        elapsed_time = time.monotonic() - start_time
        remaining_time = timeout - elapsed_time

        # Check if the total time has expired
        if remaining_time <= 0:
            my_log.log_openrouter_free(f'{log_context}: Total timeout of {timeout}s exceeded. Aborting.')
            return None

        key = get_available_key()
        if not key:
            time.sleep(5)  # Wait if no keys are available
            continue

        try:
            response = requests.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {key}",
                    "HTTP-Referer": YOUR_SITE_URL,
                    "X-Title": YOUR_APP_NAME,
                },
                data=json.dumps(payload),
                timeout=remaining_time,  # Use the remaining time for this attempt's timeout
            )

            # Handle rate limit error by freezing the key and retrying
            if response.status_code == 429 and "free-models-per-day" in response.text:
                freeze_key(key)
                my_log.log_openrouter_free(f'{log_context}: Rate limit exceeded for key. Freezing and retrying...')
                time.sleep(2)
                continue

            # Differentiate other client/server errors for logging
            if 400 <= response.status_code < 500:
                my_log.log_openrouter_free(f'{log_context}: Client error {response.status_code}. Aborting retries.\n{response.text[:500]}')
                return None  # No point in retrying client errors

            if response.status_code != 200:
                my_log.log_openrouter_free(f'{log_context}: Bad response status {response.status_code}. Retrying...\n{response.text[:500]}')
                time.sleep(5)
                continue

            json_response = None
            try:
                json_response = response.json()
                # Robust parsing using .get() to avoid KeyErrors
                base64_content = (json_response.get('choices', [{}])[0]
                                  .get('message', {})
                                  .get('images', [{}])[0]
                                  .get('image_url', {})
                                  .get('url'))

                if not base64_content:
                    if 'PROHIBITED_CONTENT' in str(response.text):
                        return None
                    # Check if there's a text response instead of an image URL.
                    # This indicates a model-level refusal or alternative reply, not a transient error.
                    if json_response:
                        text_content = (json_response.get('choices', [{}])[0]
                                        .get('message', {})
                                        .get('content'))
                        if text_content:
                            # my_log.log_openrouter_free(f'{log_context}: Received text response instead of image. Aborting.\nResponse: {text_content[:200].strip()}')
                            return None
                    my_log.log_openrouter_free(f'{log_context}: No image URL found in response. Aborting.\nResponse: {response.text[:500]}')
                    continue

                if 'base64,' in base64_content:
                    base64_content = base64_content.split('base64,', 1)[1].strip(') \n')

                if user_id:
                    my_db.add_msg(user_id, 'img ' + model)

                return base64.b64decode(base64_content)

            except (json.JSONDecodeError, IndexError, binascii.Error) as e:
                my_log.log_openrouter_free(f'{log_context}: Failed to parse/decode response: {e}\nResponse: {response.text[:500]}')
                time.sleep(5)

        except requests.exceptions.RequestException as e:
            # This will catch timeouts from requests and other connection errors
            my_log.log_openrouter_free(f'{log_context}: Request failed on attempt {attempt + 1}: {e}. Retrying...')
            time.sleep(5)

    return None


def txt2img(
    prompt: str,
    user_id: str = '',
    model: str = GEMINI25_FLASH_IMAGE,
    timeout: int = 120,
    system_prompt: str = '',
    temperature: float = 1.0,
) -> Optional[bytes]:
    """
    Generates an image from a text prompt using a specified model on OpenRouter.

    Args:
        prompt (str): The text prompt describing the desired image.
        user_id (str): The user's ID for logging purposes. Defaults to ''.
        model (str): The model to use for generation. Defaults to a free Gemini model.
        timeout (int): Request timeout in seconds. Defaults to 120.
        system_prompt (str): An optional system prompt to guide the model. Defaults to ''.
        temperature (float): The generation temperature. Defaults to 1.0.

    Returns:
        Optional[bytes]: The generated image data as bytes if successful, otherwise None.
    """
    if not model.endswith(':free'):
        my_log.log_openrouter_free(f"txt2img: Model '{model}' is not a free model.")
        return None

    messages: List[Dict[str, Any]] = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    return _send_image_request(
        model=model,
        messages=messages,
        user_id=user_id,
        timeout=timeout,
        log_context='txt2img',
        temperature=temperature
    )


def edit_image(
    prompt: str,
    source_image: Union[bytes, str, List[Union[bytes, str]]],
    user_id: str = '',
    model: str = GEMINI25_FLASH_IMAGE,
    timeout: int = 180,
    system_prompt: str = '',
    temperature: float = 1.0,
) -> Optional[bytes]:
    """
    Edits an image based on a text prompt using a specified model on OpenRouter.
    Can accept one or more source images.

    Args:
        prompt (str): The text prompt describing the desired edit.
        source_image (Union[bytes, str, List[Union[bytes, str]]]): The source image(s)
            data as bytes, a file path (str), or a list of bytes/paths.
        user_id (str): The user's ID for logging purposes. Defaults to ''.
        model (str): The model to use for generation. Defaults to a free Gemini model.
        timeout (int): Request timeout in seconds. Defaults to 180.
        system_prompt (str): An optional system prompt to guide the model. Defaults to ''.
        temperature (float): The generation temperature. Defaults to 1.0.

    Returns:
        Optional[bytes]: The edited image data as bytes if successful, otherwise None.
    """
    if not model.endswith(':free'):
        my_log.log_openrouter_free(f"edit_image: Model '{model}' is not a free model.")
        return None

    # Normalize input to always be a list of images
    if isinstance(source_image, (bytes, str)):
        image_list = [source_image]
    else:
        image_list = source_image

    # Prepare content for the API message
    content_list: List[Dict[str, Any]] = [{"type": "text", "text": prompt}]

    for image_item in image_list:
        img_bytes = image_item
        if isinstance(image_item, str):
            with open(image_item, 'rb') as f:
                img_bytes = f.read()

        base64_image = base64.b64encode(img_bytes).decode('utf-8')
        content_list.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
        })

    messages: List[Dict[str, Any]] = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": content_list})

    return _send_image_request(
        model=model,
        messages=messages,
        user_id=user_id,
        timeout=timeout,
        log_context='edit_image',
        temperature=temperature
    )


#### flash25 image generate and edit ####

if __name__ == '__main__':
    pass
    my_db.init(backup=False)

    # reset('test')
    # chat_cli('qwen/qwen3-14b:free')


    # with open(r'C:\Users\user\Downloads\samples for ai\–±–æ–ª—å—à–∞—è –∫–Ω–∏–≥–∞.txt', 'r', encoding='utf-8') as f:
    #     text = f.read()
    # q = f'–ö—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∂–∏ 32 –≥–ª–∞–≤—É\n\n {text[:600000]}'
    # r = ai(q, model = DEFAULT_MODEL_FALLBACK)
    # print(r)


    # print(img2txt('C:/Users/user/Downloads/1.jpg', '—á—Ç–æ —Ç—É—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –æ—Ç–≤–µ—Ç—å –ø–æ-—Ä—É—Å—Å–∫–∏', model='meta-llama/llama-3.2-11b-vision-instruct:free'))
    # print(img2txt(r'C:\Users\user\Downloads\samples for ai\–∫–∞—Ä—Ç–∏–Ω–∫–∏\–º–∞—Ç –∑–∞–¥–∞—á–∏.jpg', '—á—Ç–æ —Ç—É—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –æ—Ç–≤–µ—Ç—å –ø–æ-—Ä—É—Å—Å–∫–∏', model='meta-llama/llama-4-maverick:free'))
    # print(voice2txt('C:/Users/user/Downloads/1.ogg'))

    # img = txt2img('—Å–∏–≥–∞—Ä–µ—Ç–∞ –≤ —Ä—É–∫–µ –∫—Ä—É–ø–Ω—ã–º –ø–ª–∞–Ω–æ–º')
    # if img:
    #     with open(r'C:/Users/user/Downloads/1.png', 'wb') as f:
    #         f.write(img)

    # img1 = r'C:\Users\user\Downloads\samples for ai\–∫–∞—Ä—Ç–∏–Ω–∫–∏\—Å—Ç—É–¥–∏–π–Ω–æ–µ —Ñ–æ—Ç–æ —á–µ–ª–æ–≤–µ–∫–∞.png'
    # img2 = edit_image('–ø–æ–º–µ—Å—Ç–∏ –µ–≥–æ –≤–Ω—É—Ç—Ä—å —Ç–∞—Ä–¥–∏—Å, –∏ –ø—É—Å—Ç—å –æ–Ω –∫—É—Ä–∏—Ç –ø–∞–ø–∏—Ä–æ—Å—É', img1)
    # if img2:
    #     with open(r'C:/Users/user/Downloads/2.png', 'wb') as f:
    #         f.write(img2)

    # img1 = r'C:\Users\user\Downloads\samples for ai\–∫–∞—Ä—Ç–∏–Ω–∫–∏\—Å—Ç—É–¥–∏–π–Ω–æ–µ —Ñ–æ—Ç–æ —á–µ–ª–æ–≤–µ–∫–∞.png'
    # img2 = r'C:\Users\user\Downloads\samples for ai\–∫–∞—Ä—Ç–∏–Ω–∫–∏\—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è —É–ª–∏—Ü—ã.png'
    # img3 = edit_image('–ø–æ–º–µ—Å—Ç–∏ —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ —É–ª–∏—Ü–µ', (img1, img2))
    # if img3:
    #     with open(r'C:/Users/user/Downloads/3.png', 'wb') as f:
    #         f.write(img3)


    my_db.close()
