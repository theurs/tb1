#!/usr/bin/env python3

import base64
import json
import random
import requests
import time
import threading
import traceback

import cfg
import my_db
import my_log
import utils


# —Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ —Ö—Ä–∞–Ω–∏—Ç—å
MAX_MEM_LINES = 20
MAX_HIST_CHARS = 100000

# –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —á–∞—Ç–æ–≤ —á—Ç–æ –±—ã –Ω–µ –∏—Å–ø–æ—Ä—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é
# {id:lock}
LOCKS = {}

# –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã –±–æ–ª—å—à–µ —á–µ–º, —ç—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç–∞, –≤ —ç—Ç–æ–º –º–æ–¥—É–ª–µ –æ–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
MAX_REQUEST = 50000

DEFAULT_MODEL = 'qwen/qwen3-235b-a22b:free'
DEFAULT_MODEL_FALLBACK = 'qwen/qwen3-32b:free'

def clear_mem(mem, user_id: str):
    while 1:
        sizeofmem = count_tokens(mem)
        if sizeofmem <= MAX_HIST_CHARS:
            break
        try:
            mem = mem[2:]
        except IndexError:
            mem = []
            break

    return mem[-MAX_MEM_LINES*2:]


def count_tokens(mem) -> int:
    return sum([len(m['content']) for m in mem])


def ai(prompt: str = '',
       mem = None,
       user_id: str = '',
       system: str = '',
       model = DEFAULT_MODEL,
       temperature: float = 1,
       max_tokens: int = 4000,
       timeout: int = 120) -> str:

    if not model:
        model = DEFAULT_MODEL

    if not model.endswith(':free'):
        return ''

    if not prompt and not mem:
        return ''

    if not hasattr(cfg, 'OPEN_ROUTER_FREE_KEYS') or len(cfg.OPEN_ROUTER_FREE_KEYS) < 1:
        return ''

    if not temperature:
        temperature = 0.1
    if 'llama' in model and temperature > 0:
        temperature = temperature / 2

    mem_ = mem[:] if mem else []

    # current date time string
    now = utils.get_full_time()
    systems = (
        f'Current date and time: {now}\n',
        'Ask again if something is unclear in the request',
        'You (assistant) are currently working in a Telegram bot. The Telegram bot automatically extracts text from any type of files sent to you by the user, such as documents, images, audio recordings, etc., so that you can fully work with any files.',
        # 'You (assistant) can use some telegram bot functions, answer starting /tts lang text-to-say (example: /tts ru –ü—Ä–∏–≤–µ—Ç) and user will get voice message from you, answer starting /img detailed-professional-description-of-an-image and user will get images generated by ai.',
        "If the user's request cannot be fulfilled using the available tools or direct actions, the assistant(you) must treat the request as a request to generate text (e.g., providing code as text), not a request to perform an action (e.g., executing code or interacting with external systems not directly supported by tools) (intention mismatch).",
        "To edit image user can send image with caption starting ! symbol",
    )

    if system:
        mem_.insert(0, {"role": "system", "content": system})
    for s in reversed(systems):
        mem_.insert(0, {"role": "system", "content": s})
    if prompt:
        mem_ = mem_ + [{'role': 'user', 'content': prompt}]

    YOUR_SITE_URL = 'https://t.me/kun4sun_bot'
    YOUR_APP_NAME = 'kun4sun_bot'

    result = ''

    start_time = time.time()

    for _ in range(3):
        if time.time() - start_time > timeout:
            return ''
        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {random.choice(cfg.OPEN_ROUTER_FREE_KEYS)}",
                "HTTP-Referer": f"{YOUR_SITE_URL}",
                "X-Title": f"{YOUR_APP_NAME}",
            },
            data=json.dumps({
                "model": model,
                "messages": mem_,
                "max_tokens": max_tokens,
                "temperature": temperature,
            }),
            timeout = timeout,
        )

        status = response.status_code
        if status == 200:
            try:
                result = response.json()['choices'][0]['message']['content'].strip()
                break
            except Exception as error:
                my_log.log_openrouter_free(f'Failed to parse response: {error}\n\n{str(response)}')
                result = ''
                if response.text.startswith("""{"error":{"message":"This endpoint\'s maximum context length is"""):
                    return ''
                if not result and model == DEFAULT_MODEL:
                    return ai(prompt, mem, user_id, system, DEFAULT_MODEL_FALLBACK, temperature, max_tokens, timeout)
                time.sleep(2)
        else:
            my_log.log_openrouter_free(f'Bad response.status_code\n\n{str(response)[:2000]}')
            time.sleep(2)

    if not result and model == DEFAULT_MODEL:
        result = ai(prompt, mem, user_id, system, DEFAULT_MODEL_FALLBACK, temperature, max_tokens, timeout)

    return result


def update_mem(query: str, resp: str, chat_id: str):
    mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []
    mem += [{'role': 'user', 'content': query}]
    mem += [{'role': 'assistant', 'content': resp}]
    mem = clear_mem(mem, chat_id)

    mem__ = []
    try:
        i = 0
        while i < len(mem):
            if i == 0 or mem[i] != mem[i-1]:
                mem__.append(mem[i])
            i += 1
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_openrouter_free(f'my_openrouter:update_mem: {error}\n\n{error_traceback}\n\n{query}\n\n{resp}\n\n{mem}')

    my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem__))


def chat(
    query: str,
    chat_id: str = '',
    temperature: float = 1,
    system: str = '',
    model: str = ''
    ) -> str:
    global LOCKS
    if chat_id in LOCKS:
        lock = LOCKS[chat_id]
    else:
        lock = threading.Lock()
        LOCKS[chat_id] = lock
    with lock:
        mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []

        text = ai(query, mem, user_id=chat_id, temperature = temperature, system=system, model=model)

        if text:
            if DEFAULT_MODEL in model:
                my_db.add_msg(chat_id, 'llama-4-maverick')
            elif DEFAULT_MODEL_FALLBACK in model:
                my_db.add_msg(chat_id, 'llama-4-scout')
            mem += [{'role': 'user', 'content': query}]
            mem += [{'role': 'assistant', 'content': text}]
            mem = clear_mem(mem, chat_id)
            my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem))
        return text
    return ''


def chat_cli(model: str = ''):
    reset('test')
    while 1:
        q = input('>')
        if q == 'mem':
            print(get_mem_as_string('test'))
            continue
        r = chat(q, 'test', model = model, system='–æ—Ç–≤–µ—á–∞–π –≤—Å–µ–≥–¥–∞ –Ω–∞ —è–∑—ã–∫–µ')
        print(r)


def force(chat_id: str, text: str):
    '''update last bot answer with given text'''
    try:
        if chat_id in LOCKS:
            lock = LOCKS[chat_id]
        else:
            lock = threading.Lock()
            LOCKS[chat_id] = lock
        with lock:
            mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []
            if mem and len(mem) > 1:
                mem[-1]['content'] = text
                my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem))
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_openrouter_free(f'Failed to force message in chat {chat_id}: {error}\n\n{error_traceback}')


def undo(chat_id: str):
    """
    Undo the last two lines of chat history for a given chat ID.

    Args:
        chat_id (str): The ID of the chat.

    Raises:
        Exception: If there is an error while undoing the chat history.

    Returns:
        None
    """
    try:
        if chat_id in LOCKS:
            lock = LOCKS[chat_id]
        else:
            lock = threading.Lock()
            LOCKS[chat_id] = lock
        with lock:
            mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []
            # remove 2 last lines from mem
            mem = mem[:-2]
            my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem))
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_openrouter_free(f'Failed to undo chat {chat_id}: {error}\n\n{error_traceback}')


def reset(chat_id: str):
    """
    Resets the chat history for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to reset.

    Returns:
        None
    """
    mem = []
    my_db.set_user_property(chat_id, 'dialog_openrouter', my_db.obj_to_blob(mem))


def get_mem_as_string(chat_id: str, md: bool = False) -> str:
    """
    Returns the chat history as a string for the given ID.

    Parameters:
        chat_id (str): The ID of the chat to get the history for.

    Returns:
        str: The chat history as a string.
    """
    try:
        mem = my_db.blob_to_obj(my_db.get_user_property(chat_id, 'dialog_openrouter')) or []
        result = ''
        for x in mem:
            role = x['role']
            if role == 'user': role = 'ùêîùêíùêÑùêë'
            if role == 'assistant': role = 'ùêÅùêéùêì'
            if role == 'system': role = 'ùêíùêòùêíùêìùêÑùêå'
            text = x['content']
            if text.startswith('[Info to help you answer'):
                end = text.find(']') + 1
                text = text[end:].strip()
            if md:
                result += f'{role}:\n\n{text}\n\n'
            else:
                result += f'{role}: {text}\n'
            if role == 'ùêÅùêéùêì':
                if md:
                    result += '\n\n'
                else:
                    result += '\n'
        return result 
    except Exception as error:
        error_traceback = traceback.format_exc()
        my_log.log_openrouter_free(f'get_mem_as_string: {error}\n\n{error_traceback}')
        return ''


def img2txt(
    image_data: bytes,
    prompt: str = 'Describe picture',
    model = DEFAULT_MODEL,
    temperature: float = 1,
    max_tokens: int = 2000,
    timeout: int = 120,
    chat_id: str = '',
    system: str = '',
    ) -> str:
    """
    Describes an image using the specified model and parameters.

    Args:
        image_data: The image data as bytes.
        prompt: The prompt to guide the description. Defaults to 'Describe picture'.
        model: The model to use for generating the description. Defaults to DEFAULT_MODEL.
        temperature: The temperature parameter for controlling the randomness of the output. Defaults to 1.
        max_tokens: The maximum number of tokens to generate. Defaults to 2000.
        timeout: The timeout for the request in seconds. Defaults to 120.

    Returns:
        A string containing the description of the image, or an empty string if an error occurs.
    """

    if isinstance(image_data, str):
        with open(image_data, 'rb') as f:
            image_data = f.read()

    if not model:
        model = DEFAULT_MODEL

    if not model.endswith(':free'):
        return ''

    if not prompt:
        prompt = 'Describe picture'
        return ''

    if not hasattr(cfg, 'OPEN_ROUTER_FREE_KEYS'):
        return ''

    if 'llama' in model and temperature > 0:
        temperature = temperature / 2

    base64_image = base64.b64encode(image_data).decode()

    mem = [
        {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": prompt
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            }
        ]
        }
    ]
    if system:
        mem.insert(0, {'role': 'system', 'content': system})

    YOUR_SITE_URL = 'https://t.me/kun4sun_bot'
    YOUR_APP_NAME = 'kun4sun_bot'

    result = ''

    for _ in range(3):
        response = requests.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {random.choice(cfg.OPEN_ROUTER_FREE_KEYS)}",
                "HTTP-Referer": f"{YOUR_SITE_URL}",  # Optional, for including your app on openrouter.ai rankings.
                "X-Title": f"{YOUR_APP_NAME}",  # Optional. Shows in rankings on openrouter.ai.
            },
            data=json.dumps({

                "model": model,
                "temperature": temperature,
                "messages": mem,
                "max_tokens": max_tokens

            }),
            timeout=timeout,
        )

        status = response.status_code
        if status == 200:
            try:
                result = response.json()['choices'][0]['message']['content'].strip()
                break
            except Exception as error:
                my_log.log_openrouter_free(f'Failed to parse response: {error}\n\n{str(response)}')
                result = ''
                time.sleep(2)
        else:
            my_log.log_openrouter_free(f'Bad response.status_code\n\n{str(response)[:2000]}')
            time.sleep(2)
    if chat_id:
        my_db.add_msg(chat_id, model)

    if not result and model == DEFAULT_MODEL:
        result = img2txt(image_data, prompt, DEFAULT_MODEL_FALLBACK, temperature, max_tokens, timeout, chat_id, system)

    return result


# def voice2txt(
#     voice_data: bytes,
#     model = 'google/gemini-flash-8b-1.5-exp',
#     temperature: float = 0,
#     max_tokens: int = 2000,
#     timeout: int = 120) -> str:
#     """
#     Transcribes audio data to text using the specified model and parameters.

#     Args:
#         voice_data: The audio data as bytes.
#         model: The model to use for generating the transcription. Defaults to 'google/gemini-flash-8b-1.5-exp'.
#         temperature: The temperature parameter for controlling the randomness of the output. Defaults to 0.
#         max_tokens: The maximum number of tokens to generate. Defaults to 2000.
#         timeout: The timeout for the request in seconds. Defaults to 120.

#     Returns:
#         A string containing the transcribed text, or an empty string if an error occurs.
#     """

#     if isinstance(voice_data, str):
#         with open(voice_data, 'rb') as f:
#             voice_data = f.read()

#     if not model:
#         model = 'google/gemini-flash-8b-1.5-exp'

#     # if not model.endswith(':free'):
#     #     return ''

#     if not hasattr(cfg, 'OPEN_ROUTER_FREE_KEYS'):
#         return ''

#     base64_voice = base64.b64encode(voice_data).decode()

#     YOUR_SITE_URL = 'https://t.me/kun4sun_bot'
#     YOUR_APP_NAME = 'kun4sun_bot'

#     result = ''

#     for _ in range(3):
#         response = requests.post(
#             url="https://openrouter.ai/api/v1/chat/completions",
#             headers={
#                 "Authorization": f"Bearer {random.choice(cfg.OPEN_ROUTER_FREE_KEYS)}",
#                 "HTTP-Referer": f"{YOUR_SITE_URL}",  # Optional, for including your app on openrouter.ai rankings.
#                 "X-Title": f"{YOUR_APP_NAME}",  # Optional. Shows in rankings on openrouter.ai.
#             },
#             data=json.dumps({

#                 "model": model,
#                 "temperature": temperature,
#                 "messages": [
#                     {
#                     "role": "user",
#                     "content": [
#                         {
#                             "type": "text",
#                             "text": 'transcribe it'
#                         },
#                         {
#                             "type": "voice_url",
#                             "voice_url": {
#                                 "url": f"data:audio/mpeg;base64,{base64_voice}"
#                             }
#                         }
#                     ]
#                     }
#                 ],
#                 "max_tokens": max_tokens

#             }),
#             timeout=timeout,
#         )

#         status = response.status_code
#         if status == 200:
#             try:
#                 result = response.json()['text'].strip()
#                 break
#             except Exception as error:
#                 my_log.log_openrouter_free(f'Failed to parse response: {error}\n\n{str(response)}')
#                 result = ''
#                 time.sleep(2)
#         else:
#             my_log.log_openrouter_free(f'Bad response.status_code\n\n{str(response)[:2000]}')
#             time.sleep(2)

#     return result


if __name__ == '__main__':
    pass
    my_db.init(backup=False)

    # reset('test')
    # chat_cli('qwen/qwen3-14b:free')


    # with open(r'C:\Users\user\Downloads\samples for ai\–±–æ–ª—å—à–∞—è –∫–Ω–∏–≥–∞.txt', 'r', encoding='utf-8') as f:
    #     text = f.read()
    # q = f'–ö—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∂–∏ 32 –≥–ª–∞–≤—É\n\n {text[:600000]}'
    # r = ai(q, model = DEFAULT_MODEL_FALLBACK)
    # print(r)


    # print(img2txt('C:/Users/user/Downloads/1.jpg', '—á—Ç–æ —Ç—É—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –æ—Ç–≤–µ—Ç—å –ø–æ-—Ä—É—Å—Å–∫–∏', model='meta-llama/llama-3.2-11b-vision-instruct:free'))
    print(img2txt(r'C:\Users\user\Downloads\samples for ai\–∫–∞—Ä—Ç–∏–Ω–∫–∏\–º–∞—Ç –∑–∞–¥–∞—á–∏.jpg', '—á—Ç–æ —Ç—É—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –æ—Ç–≤–µ—Ç—å –ø–æ-—Ä—É—Å—Å–∫–∏', model='meta-llama/llama-4-maverick:free'))
    # print(voice2txt('C:/Users/user/Downloads/1.ogg'))

    my_db.close()
